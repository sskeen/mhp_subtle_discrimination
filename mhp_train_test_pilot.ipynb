{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2a2d5145-7d61-44b3-b692-45b10ce45ad2",
      "metadata": {
        "id": "2a2d5145-7d61-44b3-b692-45b10ce45ad2"
      },
      "source": [
        "## Linguistic markers of subtle discrimination among mental healthcare professionals"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "118c708c-2753-4071-bb5c-837ab25d0780",
      "metadata": {
        "id": "118c708c-2753-4071-bb5c-837ab25d0780"
      },
      "source": [
        "_Preprocesses, trains, tests (5-fold CV), and ranks feature importance of mental health professional (MHP) response quality targets while replying to appointment queries, using binary XGBoost classifiers. Fine tunes and evaluates (5-fold CV) rationale-augmented MHP response quality targets across BERT, RoBERTa, and DistilBERT pretrained LMs._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3dd7d28-46f5-4c73-a4a9-4b4613295ec7",
      "metadata": {
        "id": "a3dd7d28-46f5-4c73-a4a9-4b4613295ec7"
      },
      "source": [
        "> mhp_train_test_pilot.ipynb<br>\n",
        "> Simone J. Skeen (06-25-2024)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install contractions\n",
        "%pip install simpletransformers\n",
        "%pip install unidecode\n",
        "%pip install wandb\n",
        "\n",
        "!python -m spacy download en_core_web_lg --user"
      ],
      "metadata": {
        "id": "e2DaWUbFLnhl",
        "collapsed": true
      },
      "id": "e2DaWUbFLnhl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions\n",
        "import logging\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "import spacy\n",
        "import string\n",
        "import wandb.sdk\n",
        "import warnings\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import Counter\n",
        "from functools import reduce\n",
        "from google.colab import drive\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, KFold, RepeatedStratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score, average_precision_score, matthews_corrcoef\n",
        "from textblob import TextBlob\n",
        "from unidecode import unidecode\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = 'all'\n",
        "\n",
        "pd.set_option(\n",
        "              'display.max_columns',\n",
        "              None,\n",
        "              )\n",
        "\n",
        "pd.set_option(\n",
        "              'display.max_rows',\n",
        "              None,\n",
        "              )\n",
        "\n",
        "warnings.simplefilter(\n",
        "                      action = 'ignore',\n",
        "                      category = FutureWarning,\n",
        "                      )\n",
        "\n",
        "#!python -m prodigy stats"
      ],
      "metadata": {
        "id": "Tq8uXi_4LxT2"
      },
      "id": "Tq8uXi_4LxT2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\n",
        "            '/content/gdrive/',\n",
        "            force_remount = True,\n",
        "            )\n",
        "\n",
        "%cd gdrive/My Drive/Colab/mhp_subtle_discrimination/data"
      ],
      "metadata": {
        "id": "yoa5ilNUOivV"
      },
      "id": "yoa5ilNUOivV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Pre-annotation\n",
        "***"
      ],
      "metadata": {
        "id": "EJRze3oKGX3q"
      },
      "id": "EJRze3oKGX3q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**_Import_**"
      ],
      "metadata": {
        "id": "vS9NDYhKGXtA"
      },
      "id": "vS9NDYhKGXtA"
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "d = pd.read_csv(\n",
        "                'Therapy Discrimination - Data collection file - Response Data.csv',\n",
        "                encoding = 'utf8',\n",
        "                header = 1,\n",
        "                )\n",
        "\n",
        "print(len(d.columns))\n",
        "for col in d.columns:\n",
        "    print(col)\n",
        "d.head(3)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "HtqMs7FuM3AS"
      },
      "id": "HtqMs7FuM3AS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**_Reformat_**"
      ],
      "metadata": {
        "id": "ew3yQx-CTmGG"
      },
      "id": "ew3yQx-CTmGG"
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce\n",
        "\n",
        "d = d[[\n",
        "       'Response (Y/N)',\n",
        "       'Email Pair ID#',\n",
        "       'Within Patient ID',\n",
        "       'First Email in Pair? (Y/N)',\n",
        "       'Email, Phone, or Text (E/P/T)',\n",
        "       'Offered a pre-appt consultation/to talk on the phone? (Y/N)',\n",
        "       'Implied or explicit appointment offer? (Y/N)',\n",
        "       'Rejection? (Y/N)',\n",
        "       'Asked about insurance/payment? (Y/N)',\n",
        "       'Asked about trans/nonbinary issues? (Y/N)',\n",
        "       'Copy-and-Paste Email Response of Voicemail (if called but no voicemail, leave empty)',\n",
        "       'Outcome Recode (A = Appointment, C = Call offer, Q = Screening questions, W = Waitlist, R = Referral, X = Rejection, NV = no voice message, N = No response, I = Inconclusive)\\n',\n",
        "       'Secondary Outcome Recode (A = Appointment, C = Call offer, Q = Screening questions, W = Waitlist, R = Referral, X = Rejection, NV = no voice message, N = No response, I = Inconclusive)',\n",
        "       ]]\n",
        "\n",
        "# Rename\n",
        "\n",
        "d.rename(\n",
        "         columns = {\n",
        "                    'Response (Y/N)': 'response',\n",
        "                    'Email Pair ID#': 'email_pair_id',\n",
        "                    'Within Patient ID': 'client_id',\n",
        "                    'First Email in Pair? (Y/N)': 'first_email',\n",
        "                    'Email, Phone, or Text (E/P/T)': 'ept',\n",
        "                    'Offered a pre-appt consultation/to talk on the phone? (Y/N)': 'consult',\n",
        "                    'Implied or explicit appointment offer? (Y/N)': 'appointment',\n",
        "                    'Rejection? (Y/N)': 'reject',\n",
        "                    'Asked about insurance/payment? (Y/N)' : 'tnb_ask',\n",
        "                    'Asked about trans/nonbinary issues? (Y/N)' : 'ins_ask',\n",
        "                    'Copy-and-Paste Email Response of Voicemail (if called but no voicemail, leave empty)': 'text',\n",
        "                    'Outcome Recode (A = Appointment, C = Call offer, Q = Screening questions, W = Waitlist, R = Referral, X = Rejection, NV = no voice message, N = No response, I = Inconclusive)\\n': 'prmr_outcome',\n",
        "                    'Secondary Outcome Recode (A = Appointment, C = Call offer, Q = Screening questions, W = Waitlist, R = Referral, X = Rejection, NV = no voice message, N = No response, I = Inconclusive)': 'scnd_outcome',\n",
        "                    }, inplace = True,\n",
        "        )\n",
        "\n",
        "# Restrict: response = 1\n",
        "\n",
        "d = d[d['response'] == 1.0]\n",
        "\n",
        "# Encode 'Y/N' string\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "outcomes = [\n",
        "            'consult',\n",
        "            'appointment',\n",
        "            'reject',\n",
        "            'tnb_ask',\n",
        "            'ins_ask',\n",
        "            ]\n",
        "\n",
        "for outcome in outcomes:\n",
        "    if d[outcome].isnull().any():\n",
        "        d[outcome].fillna(\n",
        "                          'unknown',\n",
        "                          inplace = True,\n",
        "                          )\n",
        "\n",
        "    d[outcome] = le.fit_transform(d[outcome])\n",
        "    d[outcome].replace({\n",
        "                        2: 1,\n",
        "                        1: 0,\n",
        "                            }, inplace = True,\n",
        "                       )\n",
        "\n",
        "d.shape\n",
        "d.dtypes\n",
        "d.head(3)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dDD1ZJkYOCLz"
      },
      "id": "dDD1ZJkYOCLz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**_Preprocess_**"
      ],
      "metadata": {
        "id": "5DteHCVCTy6R"
      },
      "id": "5DteHCVCTy6R"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define spaCy NE redaction Fx\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "def redact_ne(mhp_text):\n",
        "    '''replaces named entities with <|PII|>'''\n",
        "    ne = list(\n",
        "              [\n",
        "               'PERSON',   ### people, including fictional\n",
        "               'NORP',     ### nationalities or religious or political groups\n",
        "               'FAC',      ### buildings, airports, highways, bridges, etc.\n",
        "               'ORG',      ### companies, agencies, institutions, etc.\n",
        "               'GPE',     ### countries, cities, states\n",
        "               'LOC',      ### non-GPE locations, mountain ranges, bodies of water\n",
        "               'PRODUCT',  ### objects, vehicles, foods, etc. (not services)\n",
        "               'EVENT',    ### named hurricanes, battles, wars, sports events, etc.\n",
        "               ]\n",
        "                )\n",
        "\n",
        "    doc = nlp(mhp_text)\n",
        "    ne_to_remove = []\n",
        "    final_string = str(mhp_text)\n",
        "    for sent in doc.ents:\n",
        "        if sent.label_ in ne:\n",
        "            ne_to_remove.append(str(sent.text))\n",
        "    for n in range(len(ne_to_remove)):\n",
        "        final_string = final_string.replace(\n",
        "                                            ne_to_remove[n],\n",
        "                                            '<|PII|>',\n",
        "                                            )\n",
        "    return final_string"
      ],
      "metadata": {
        "id": "msb3MF3QVgHc"
      },
      "id": "msb3MF3QVgHc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NER redaction\n",
        "\n",
        "d['text'] = d['text'].astype(str).apply(lambda i: redact_ne(i))\n",
        "\n",
        "# Excise numerals\n",
        "\n",
        "d['text'] = d['text'].apply(lambda i: re.sub(\n",
        "                                             r'\\d+',\n",
        "                                             ' ',\n",
        "                                             i)\n",
        "                            )\n",
        "\n",
        "# Harmonize manual redactions\n",
        "\n",
        "redactions = [\n",
        "              '[PATIENT NAME]',\n",
        "              '[MHP NAME]',\n",
        "              '[CITY]',\n",
        "              ]\n",
        "\n",
        "d['text'] = d['text'].astype(str).apply(lambda i: reduce(\n",
        "                                                         lambda s, r: s.replace(\n",
        "                                                                                r,\n",
        "                                                                                '<|PII|>',\n",
        "                                                                                ), redactions, i\n",
        "                                                         )\n",
        "                                        )\n",
        "\n",
        "\n",
        "                           )\n",
        "\n",
        "d['text'] = d['text'].astype(str).apply(lambda i: i.replace(\n",
        "                                                            '\\n',\n",
        "                                                            ' ',\n",
        "                                                            )\n",
        "                                        )\n",
        "\n",
        "d.head(3)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4bm0k0VWTxs3"
      },
      "id": "4bm0k0VWTxs3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d.to_excel('d_anon.xlsx')"
      ],
      "metadata": {
        "id": "tAXWiX2NUV3T"
      },
      "id": "tAXWiX2NUV3T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lnxBmCOOUV8j"
      },
      "id": "lnxBmCOOUV8j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PwX5zX8JUWAx"
      },
      "id": "PwX5zX8JUWAx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_R6YJgbZUWEp"
      },
      "id": "_R6YJgbZUWEp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4e2ltKMGUWJO"
      },
      "id": "4e2ltKMGUWJO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xZCXO012UWNb"
      },
      "id": "xZCXO012UWNb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post-annotation\n",
        "***"
      ],
      "metadata": {
        "id": "56RZyYisUXlt"
      },
      "id": "56RZyYisUXlt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a162fb7-e828-41d0-ba6c-d126adaecdfe",
      "metadata": {
        "id": "9a162fb7-e828-41d0-ba6c-d126adaecdfe"
      },
      "outputs": [],
      "source": [
        "# Replace NaN\n",
        "\n",
        "targets = [\n",
        "           'prob', ### prob = _not_ coherent target; exploratory-qualitative only\n",
        "           'refl',\n",
        "           'just',\n",
        "           'afrm',\n",
        "           'fit',\n",
        "           'agnt',\n",
        "           ]\n",
        "\n",
        "for target in targets:\n",
        "    d[target].fillna(\n",
        "                     0,\n",
        "                     inplace = True,\n",
        "                     )\n",
        "\n",
        "d.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11025141-9e4a-406b-b7d4-5b24480540b2",
      "metadata": {
        "id": "11025141-9e4a-406b-b7d4-5b24480540b2"
      },
      "outputs": [],
      "source": [
        "# Encode audit study outcomes\n",
        "\n",
        "        ### _note_: exploratory\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "outcomes = [\n",
        "            'consult',\n",
        "            'appointment',\n",
        "            'reject',\n",
        "            ]\n",
        "\n",
        "\n",
        "for outcome in outcomes:\n",
        "    if d[outcome].isnull().any():\n",
        "        d[outcome].fillna('unknown', inplace = True) ### NaNs recoded\n",
        "\n",
        "    d[outcome] = le.fit_transform(d[outcome])\n",
        "    d[outcome].replace({\n",
        "                        2: 1,\n",
        "                        1: 0,\n",
        "                            }, inplace = True)\n",
        "\n",
        "d.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21e015df-b177-4a33-ab3f-af737f95d9c0",
      "metadata": {
        "id": "21e015df-b177-4a33-ab3f-af737f95d9c0"
      },
      "outputs": [],
      "source": [
        "# Drop pilot preprocessing\n",
        "\n",
        "        ### _note_ 3/26: drop exploratorily preprocessed data from pilot runs\n",
        "\n",
        "d.drop(\n",
        "       'text_response_pre',\n",
        "       axis = 1,\n",
        "       inplace = True,\n",
        "       )\n",
        "\n",
        "d.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed546565-e9d8-482a-86f0-39910c5cffd2",
      "metadata": {
        "id": "ed546565-e9d8-482a-86f0-39910c5cffd2"
      },
      "outputs": [],
      "source": [
        "# Drop annotation artifacts\n",
        "\n",
        "artifacts = [\n",
        "             '<PII>', ### <PII> _only_ in non-augmented data\n",
        "             #'<PROB>',\n",
        "             #'<JUST>',\n",
        "             #'<AFRM>',\n",
        "             #'<FIT>',\n",
        "             #'<AGNT>',\n",
        "             #'<REFL>',\n",
        "             ]\n",
        "\n",
        "for artifact in artifacts:\n",
        "    d['text_response_anon'] = d['text_response_anon'].str.replace(artifact, ' ', regex = True)\n",
        "\n",
        "        ### SJS 3/24: artifacts = perfect 1:1 x_pred; must excise in annotated data\n",
        "\n",
        "d.to_csv('d_inspect.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e4fa89c-0e39-4086-8e24-4a491b3ca5ba",
      "metadata": {
        "id": "6e4fa89c-0e39-4086-8e24-4a491b3ca5ba"
      },
      "outputs": [],
      "source": [
        "# Verify\n",
        "\n",
        "d.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f4e9fd7-1fa9-4873-9557-ef57f9286499",
      "metadata": {
        "tags": [],
        "id": "9f4e9fd7-1fa9-4873-9557-ef57f9286499"
      },
      "outputs": [],
      "source": [
        "# Preprocess\n",
        "\n",
        "# Convert to lowercase\n",
        "\n",
        "t_col = d['text_response_anon'].astype(str).apply(lambda i: i.lower())\n",
        "\n",
        "# Expand contractions\n",
        "\n",
        "t_col = t_col.apply(lambda i: ' '.join([contractions.fix(expanded_word) for expanded_word in i.split()]))\n",
        "\n",
        "# Excise numbers\n",
        "\n",
        "t_col = t_col.apply(lambda i: re.sub(r'\\d+', ' ', i))\n",
        "\n",
        "# Excise punctuation\n",
        "\n",
        "t_col = t_col.apply(lambda i: re.sub('[%s]' % re.escape(string.punctuation), ' ' , i))\n",
        "\n",
        "# Convert diacriticals\n",
        "\n",
        "t_col = t_col.apply(lambda i: unidecode(i, errors = 'preserve'))\n",
        "\n",
        "        ### _note_ 10/5: errors = 'preserve': retain if no replacement character possible\n",
        "\n",
        "# Standardize/correct spelling\n",
        "\n",
        "t_col = t_col.apply(lambda i: str(TextBlob(i).correct()))\n",
        "\n",
        "        ### SJS 3/19: ~5 min runtime...\n",
        "\n",
        "# Update stoplist\n",
        "\n",
        "sw_nltk = stopwords.words('english')\n",
        "sw_add = [\n",
        "          'um',\n",
        "          ]\n",
        "\n",
        "sw_nltk.extend(sw_add)\n",
        "\n",
        "# Apply updated stoplist\n",
        "\n",
        "d['text_response_pre'] = t_col.apply(lambda i: ' '.join([ word for word in i.split() if word not in sw_nltk]))\n",
        "\n",
        "# Inspect\n",
        "\n",
        "d.head(3)\n",
        "d.to_csv('d_inspect.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac694110-9d01-450f-b4ad-a37b37e09231",
      "metadata": {
        "tags": [],
        "id": "ac694110-9d01-450f-b4ad-a37b37e09231"
      },
      "outputs": [],
      "source": [
        "# Contextualize artifacts from pilot runs\n",
        "\n",
        "d_inspect = pd.read_csv(\n",
        "                        'd_inspect.csv',\n",
        "                        encoding = 'unicode_escape',\n",
        "                        header = 0,\n",
        "                        )\n",
        "\n",
        "\n",
        "# Unstandardized text: 'um'\n",
        "\n",
        "d_inspect['text_response_pre'] = d_inspect['text_response_pre'].apply(lambda i: nltk.word_tokenize(str(i)) if i is not None else [])\n",
        "nltk_t = nltk.Text(word for tokens in d['text_response_pre'] for word in tokens)\n",
        "\n",
        "print(nltk_t.concordance('um', width = 120, lines = 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccdac3ba-1c67-4793-8694-133443847171",
      "metadata": {
        "tags": [],
        "id": "ccdac3ba-1c67-4793-8694-133443847171"
      },
      "outputs": [],
      "source": [
        "# Count vectorize\n",
        "\n",
        "        ### _note_ 3/7: accomodating feature importance downstream...\n",
        "\n",
        "#from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#cv = CountVectorizer(\n",
        "#                     ngram_range = (1, 3),\n",
        "#                     min_df = 0.01, ### ignore tokens that appear in <1% of responses\n",
        "                     #stop_words = 'english',\n",
        "#                     )\n",
        "\n",
        "#cv_matrix = cv.fit_transform(d['text_response_pre'].values.astype('U'))\n",
        "\n",
        "#cv_d = pd.DataFrame(cv_matrix.toarray(), columns = cv.get_feature_names_out())\n",
        "#cv_d.shape\n",
        "\n",
        "#cv_features = pd.concat([d, cv_d], axis = 1) ### f = features\n",
        "\n",
        "#cv_features.head(3)\n",
        "\n",
        "        ### _note_ 3/7: export, c/b of use\n",
        "\n",
        "#cv_features.to_csv('mh_document-feature_matrix.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f147248c-3ac4-4a24-9777-0788f4db7b7a",
      "metadata": {
        "id": "f147248c-3ac4-4a24-9777-0788f4db7b7a"
      },
      "outputs": [],
      "source": [
        "#tfidf vectorize\n",
        "\n",
        "tv = TfidfVectorizer(\n",
        "                     ngram_range = (1, 3),\n",
        "                     min_df = 0.01, ### ignore tokens that appear in <1% of responses\n",
        "                     #stop_words = 'english',\n",
        "                     )\n",
        "\n",
        "tf_matrix = tv.fit_transform(d['text_response_pre'].values.astype('U'))\n",
        "\n",
        "tf_d = pd.DataFrame(tf_matrix.toarray(), columns = tv.get_feature_names_out())\n",
        "tf_d.shape\n",
        "\n",
        "\n",
        "tf_features = pd.concat([d, tf_d], axis = 1)\n",
        "\n",
        "tf_features.head(3)\n",
        "\n",
        "#tf_features.to_csv('mh_document-feature_matrix.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "089c283c-8431-4433-86c9-1b139b8b1fe5",
      "metadata": {
        "id": "089c283c-8431-4433-86c9-1b139b8b1fe5"
      },
      "outputs": [],
      "source": [
        "# Value counts\n",
        "\n",
        "d[[\n",
        "   'refl',\n",
        "   'just',\n",
        "   'afrm',\n",
        "   'fit',\n",
        "   'agnt',\n",
        "   ]].apply(pd.Series.value_counts)\n",
        "\n",
        "        ### _note_ 3/26: computed for '_QUAL' (non-augmented) df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89fb1ba8-68f5-4f63-b79b-62246a75288c",
      "metadata": {
        "tags": [],
        "id": "89fb1ba8-68f5-4f63-b79b-62246a75288c"
      },
      "outputs": [],
      "source": [
        "# Weights\n",
        "\n",
        "'refl: p_w'\n",
        "674 / 81 # p_w = 8.3210\n",
        "\n",
        "'just: p_w'\n",
        "709 / 46 # p_w = 15.4130\n",
        "\n",
        "'afrm: p_w'\n",
        "709 / 46 # p_w = 15.4130\n",
        "\n",
        "'fit: p_w'\n",
        "699 / 56 # p_w = 12.4821\n",
        "\n",
        "'agnt: p_w'\n",
        "715 / 40 # p_w = 17.8750\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d47692cb-e18e-4afc-85bf-4183039b2c06",
      "metadata": {
        "tags": [],
        "id": "d47692cb-e18e-4afc-85bf-4183039b2c06"
      },
      "outputs": [],
      "source": [
        "# Dummy code augmented rows\n",
        "\n",
        "d['augment'] = 0\n",
        "\n",
        "t_indices = d['rationale'].apply(lambda i: isinstance(i, str))\n",
        "\n",
        "d.loc[t_indices.shift(1, fill_value = False), 'augment'] = 1\n",
        "\n",
        "        ### _note_ 3/21: use for 'drop if' prior to eval to avoid inflated metrics - annotated data _only_\n",
        "\n",
        "d.head(10)\n",
        "#d.to_csv('d_inspect.csv', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce52514b-6b4f-45d6-83c5-65dbe2fd9989",
      "metadata": {
        "id": "ce52514b-6b4f-45d6-83c5-65dbe2fd9989"
      },
      "source": [
        "### XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43f555ef-61f0-4e36-baf7-1f39bf8dd00a",
      "metadata": {
        "tags": [],
        "id": "43f555ef-61f0-4e36-baf7-1f39bf8dd00a"
      },
      "source": [
        "**rskf train-test-feature loop: _not_ augmented**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1536afcb-c9c0-40e1-b47f-f448349baf6e",
      "metadata": {
        "tags": [],
        "id": "1536afcb-c9c0-40e1-b47f-f448349baf6e"
      },
      "outputs": [],
      "source": [
        "\n",
        "targets = [\n",
        "           'refl',\n",
        "           'just',\n",
        "           'afrm',\n",
        "           'fit',\n",
        "           'agnt',\n",
        "           ]\n",
        "\n",
        "# Scale pos weights: n neg / n pos\n",
        "\n",
        "p_w = {\n",
        "       'refl': 8.3210,\n",
        "       'just': 15.4130,\n",
        "       'afrm': 15.4130,\n",
        "       'fit':  12.4821,\n",
        "       'agnt': 17.8750,\n",
        "       }\n",
        "\n",
        "\n",
        "for target in targets:\n",
        "    '''Preprocesses, trains, tests (5-fold CV), and ranks feature importance of mental health professional (MHP)\n",
        "    response quality targets while replying to appointment queries, using binary XGBoost classifiers.'''\n",
        "\n",
        "    X = d['text_response_pre'].astype(str)\n",
        "    y = d[target]\n",
        "\n",
        "    # Vectorize\n",
        "\n",
        "    #cv = CountVectorizer(\n",
        "                         #binary = True,\n",
        "                         #ngram_range = (1, 3),\n",
        "                         #min_df = 0.01,\n",
        "                         #stop_words = 'english',\n",
        "                         #)\n",
        "\n",
        "    tv = TfidfVectorizer(\n",
        "                         ngram_range = (1, 3),\n",
        "                         min_df = 0.01,\n",
        "                         #stop_words = 'english',\n",
        "                         )\n",
        "\n",
        "    #X_count = cv.fit_transform(X)\n",
        "    X_tfidf = tv.fit_transform(X)\n",
        "\n",
        "    # Convert sparse matrix to dense array\n",
        "\n",
        "    #X_count = X_count.toarray()\n",
        "    X_tfidf = X_tfidf.toarray()\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "                                                        #X_count,\n",
        "                                                        X_tfidf,\n",
        "                                                        y,\n",
        "                                                        test_size = 0.2,\n",
        "                                                        stratify = y,\n",
        "                                                        random_state = 56,\n",
        "                                                        )\n",
        "\n",
        "    # Scale\n",
        "\n",
        "    s = StandardScaler()\n",
        "\n",
        "        ### _note_ 3/7: _fit_ on training set only\n",
        "\n",
        "    X_train_scaled = s.fit_transform(X_train)\n",
        "    X_test_scaled = s.transform(X_test)\n",
        "\n",
        "    # Classifier params\n",
        "\n",
        "    xgb = XGBClassifier(\n",
        "                        scale_pos_weight = p_w[target],\n",
        "                        #verbosity = 0,\n",
        "                        )\n",
        "\n",
        "    # Cross validate\n",
        "\n",
        "    n_splits = 5\n",
        "    n_repeats = 5\n",
        "    rskf = RepeatedStratifiedKFold(\n",
        "                                   n_splits = n_splits,\n",
        "                                   n_repeats = n_repeats,\n",
        "                                   random_state = 56,\n",
        "                                   )\n",
        "\n",
        "    # Lists to store eval metrics\n",
        "\n",
        "    f1_macro = []\n",
        "    auprc = []\n",
        "    mcc = []\n",
        "\n",
        "    # Iterate over k-fold splits\n",
        "\n",
        "    for train_index, val_index in rskf.split(X_train_scaled, y_train):\n",
        "        X_train_cv, X_val_cv = X_train_scaled[train_index], X_train_scaled[val_index]\n",
        "        y_train_cv, y_val_cv = y_train.iloc[train_index], y_train.iloc[val_index]\n",
        "\n",
        "        # Train\n",
        "\n",
        "        xgb.fit(X_train_cv, y_train_cv)\n",
        "\n",
        "        # Predict on validation set\n",
        "\n",
        "        y_pred_val = xgb.predict(X_val_cv)\n",
        "\n",
        "        # F1 macro, AUPRC, MCC eval\n",
        "\n",
        "        f1_macro.append(f1_score(y_val_cv, y_pred_val, average = 'macro'))\n",
        "        auprc.append(average_precision_score(y_val_cv, xgb.predict_proba(X_val_cv)[:, 1], average = 'macro'))\n",
        "        mcc.append(matthews_corrcoef(y_val_cv, y_pred_val))\n",
        "\n",
        "    print('----------------------------------------------------------------------------------------')\n",
        "    print(f'\\nTarget: {target}')\n",
        "    print(f'Average F1 Macro: {sum(f1_macro) / len(f1_macro)}')\n",
        "    print(f'Average AUPRC: {sum(auprc) / len(auprc)}')\n",
        "    print(f'Average MCC: {sum(mcc) / len(mcc)}')\n",
        "\n",
        "    # Plot feature importances\n",
        "\n",
        "    #print('----------------------------------------------------------------------------------------')\n",
        "    #plt.figure(figsize = (20, 260))\n",
        "    #sorted_idx = xgb.feature_importances_.argsort()\n",
        "\n",
        "        ### SJS 3/7: _NOTE_'tf_d' assigned in cell above\n",
        "\n",
        "    #plt.barh(tf_d.columns[sorted_idx], xgb.feature_importances_[sorted_idx], color = 'skyblue')\n",
        "    #plt.xlabel(\"XGBoost Feature Importance\")\n",
        "\n",
        "    #plt.title(f'Feature Importance - {target}')\n",
        "    #plt.show()\n",
        "\n",
        "    # List feature importances\n",
        "\n",
        "    print('----------------------------------------------------------------------------------------')\n",
        "    feature_names = tv.get_feature_names()\n",
        "    importance_scores = xgb.feature_importances_\n",
        "\n",
        "    feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance_scores})\n",
        "    feature_importance_df = feature_importance_df.sort_values(by = 'Importance', ascending=False)\n",
        "\n",
        "    top_20_features = feature_importance_df.head(20)\n",
        "    print(f'\\nTop 20 features for {target}:')\n",
        "    print(top_20_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c256667-1759-46d5-9ea2-1911405bee28",
      "metadata": {
        "id": "5c256667-1759-46d5-9ea2-1911405bee28"
      },
      "source": [
        "### BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "414d351e-53bc-4b27-bf0c-ba66243f978c",
      "metadata": {
        "id": "414d351e-53bc-4b27-bf0c-ba66243f978c"
      },
      "outputs": [],
      "source": [
        "os.chdir('<my_dir>')\n",
        "#%pwd\n",
        "\n",
        "# Import\n",
        "\n",
        "        ### _note_ 3/7: mh_audit_wave1_QUAL = single-annotated, n = 755\n",
        "\n",
        "        ### _note_ 3/21: mh_audit_wave1_QUAL_aug = single-annotated, _manually_ augmented w/ rationales, n = 990\n",
        "\n",
        "d = pd.read_csv(\n",
        "                'mh_audit_wave1_QUAL_aug.csv',\n",
        "                encoding = 'unicode_escape',\n",
        "                header = 0,\n",
        "                )\n",
        "\n",
        "# Replace NaN\n",
        "\n",
        "targets = [\n",
        "           'prob', ### prob = _not_ coherent target; expl only\n",
        "           'refl',\n",
        "           'just',\n",
        "           'afrm',\n",
        "           'fit',\n",
        "           'agnt',\n",
        "           ]\n",
        "\n",
        "for target in targets:\n",
        "    d[target].fillna(0, inplace = True)\n",
        "\n",
        "# Dummy code augmented rows\n",
        "\n",
        "d['augment'] = 0\n",
        "\n",
        "t_indices = d['rationale'].apply(lambda i: isinstance(i, str))\n",
        "\n",
        "d.loc[t_indices.shift(1, fill_value = False), 'augment'] = 1\n",
        "\n",
        "# Drop annotation artifacts\n",
        "\n",
        "artifacts = [\n",
        "             '<PII>',\n",
        "             '<PROB>',\n",
        "             '<JUST>',\n",
        "             '<AFRM>',\n",
        "             '<FIT>',\n",
        "             '<AGNT>',\n",
        "             '<REFL>',\n",
        "             ]\n",
        "\n",
        "for artifact in artifacts:\n",
        "    d['text_response_aug'] = d['text_response_aug'].str.replace(artifact, ' ', regex = True)\n",
        "\n",
        "d.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90f5f4f2-9229-46f3-bac8-8d3520a85250",
      "metadata": {
        "id": "90f5f4f2-9229-46f3-bac8-8d3520a85250"
      },
      "outputs": [],
      "source": [
        "# SimpleTransformer prep\n",
        "\n",
        "#d.head(3)\n",
        "#d.columns\n",
        "d['text'] = d['text_response_aug'].str.replace(r'<PII>', ' ', regex = True)\n",
        "d.drop(\n",
        "       columns = [\n",
        "                  'id',\n",
        "                  'response',\n",
        "                  'emailpair',\n",
        "                  'pid',\n",
        "                  'firstemail',\n",
        "                  'ept',\n",
        "                  'consult',\n",
        "                  'appointment',\n",
        "                  'reject',\n",
        "                  'text_response_anon',\n",
        "                  'prob',\n",
        "                  'rationale',\n",
        "                  'notes',\n",
        "                  'prmr_outcome',\n",
        "                  'scnd_outcome',\n",
        "                  #'text_response_pre',\n",
        "                  ], inplace = True)\n",
        "\n",
        "st_format = [\n",
        "             'text',\n",
        "             'augment',\n",
        "             'refl',\n",
        "             'just',\n",
        "             'afrm',\n",
        "             'fit',\n",
        "             'agnt',\n",
        "             ]\n",
        "\n",
        "d = d[st_format]\n",
        "d.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5411749b-366f-4480-b30b-20b10d8578a6",
      "metadata": {
        "id": "5411749b-366f-4480-b30b-20b10d8578a6"
      },
      "outputs": [],
      "source": [
        "d.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94f8abaf-f170-4d8e-ac66-9515af5752a7",
      "metadata": {
        "tags": [],
        "id": "94f8abaf-f170-4d8e-ac66-9515af5752a7"
      },
      "outputs": [],
      "source": [
        "# Class weights: N / n classes * n\n",
        "\n",
        "c_w = {\n",
        "       'refl': {\n",
        "                '0': 0.5978,\n",
        "                '1': 3.0556,\n",
        "                },\n",
        "       'just': {\n",
        "                '0': 0.5525,\n",
        "                '1': 5.2660,\n",
        "                },\n",
        "       'afrm': {\n",
        "                '0': 0.5518,\n",
        "                '1': 5.3226,\n",
        "                },\n",
        "       'fit': {\n",
        "                '0': 0.5644,\n",
        "                '1': 4.3805,\n",
        "                },\n",
        "       'agnt': {\n",
        "                '0': 0.5464,\n",
        "                '1': 5.8929,\n",
        "                }\n",
        "       }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06d6e85d",
      "metadata": {
        "scrolled": true,
        "id": "06d6e85d"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# View defaults\n",
        "\n",
        "'BERT defaults'\n",
        "bert_uncased = ClassificationModel(\n",
        "                                   'bert',\n",
        "                                   'bert-large-uncased',\n",
        "                                   use_cuda = False,\n",
        "                                   )\n",
        "bert_uncased.args\n",
        "\n",
        "'RoBERTa defaults'\n",
        "roberta_base = ClassificationModel(\n",
        "                                    'roberta',\n",
        "                                    'roberta-base',\n",
        "                                    use_cuda = False,\n",
        "                                    )\n",
        "roberta_base.args\n",
        "\n",
        "'DistilBERT defaults'\n",
        "distilbert_base = ClassificationModel(\n",
        "                                      'distilbert',\n",
        "                                      'distilbert-base-cased',\n",
        "                                      use_cuda = False,\n",
        "                                      )\n",
        "distilbert_base.args"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e148d253-f972-4fa3-bb85-79f7f4e23982",
      "metadata": {
        "id": "e148d253-f972-4fa3-bb85-79f7f4e23982"
      },
      "source": [
        "**kf train-test loop: _augmented_**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c248690f-e293-43ec-b796-7518d0ec6110",
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "c248690f-e293-43ec-b796-7518d0ec6110"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Fx iterates over models, args, subconstructs, class weights\n",
        "\n",
        "def train_and_evaluate_model(model_type, model_name, target, class_weights, train_d, eval_d):\n",
        "    '''Fine tunes and evaluates (5-fold CV) rationale-augmented MHP response quality targets across BERT, RoBERTa, and\n",
        "    DistilBERT pretrained LMs.'''\n",
        "\n",
        "    print('======================================================================================')\n",
        "    print(f\"\\nTraining {model_type}:{model_name}, target **{target}**\")\n",
        "    print('======================================================================================')\n",
        "\n",
        "    # Args\n",
        "\n",
        "    model_args = ClassificationArgs(\n",
        "                                    num_train_epochs = 2,\n",
        "                                    sliding_window = False, ### ~2x runtime w/ True; 'jagged arrays' incompatible w/ loop\n",
        "                                    #do_lower_case = True,\n",
        "                                    #reprocess_input_data = False,\n",
        "                                    overwrite_output_dir = True,\n",
        "                                    manual_seed = 56,\n",
        "                                    )\n",
        "\n",
        "    # Initialize model(s)\n",
        "\n",
        "    model = ClassificationModel(\n",
        "                                model_type = model_type,\n",
        "                                model_name = model_name,\n",
        "                                use_cuda = False,\n",
        "                                num_labels = 2,\n",
        "                                weight = class_weights[target],\n",
        "                                args = model_args,\n",
        "                                )\n",
        "\n",
        "    # Train\n",
        "\n",
        "    model.train_model(train_d)\n",
        "\n",
        "    print(f\"Evaluating {model_type} model for {target}\")\n",
        "\n",
        "    # Eval\n",
        "\n",
        "    result, model_outputs, _ = model.eval_model(eval_d)\n",
        "\n",
        "    # Metric\n",
        "\n",
        "    predictions = np.argmax(model_outputs, axis = 1)\n",
        "    f1_macro = f1_score(eval_d[target], predictions, average = 'macro')\n",
        "\n",
        "    print('----------------------------------------------------------------------------------------')\n",
        "    print(f\"F1 macro: {model_type} for {target} = {f1_macro}\")\n",
        "    print(\"Evaluation:\")\n",
        "    print(result)\n",
        "\n",
        "# Model types, model names\n",
        "\n",
        "model_types = [\n",
        "               'bert',\n",
        "               'roberta',\n",
        "               'distilbert',\n",
        "                ]\n",
        "\n",
        "model_names = [\n",
        "               'bert-base-cased',\n",
        "               'roberta-base',\n",
        "               'distilbert-base-cased',\n",
        "                ]\n",
        "\n",
        "# Class weights\n",
        "\n",
        "class_weights = {\n",
        "                 'refl': [\n",
        "                          0.5978,\n",
        "                          3.0556,\n",
        "                          ],\n",
        "                 'just': [\n",
        "                          0.5525,\n",
        "                          5.2660,\n",
        "                          ],\n",
        "                 'afrm': [\n",
        "                          0.5518,\n",
        "                          5.3226,\n",
        "                          ],\n",
        "                 'fit': [\n",
        "                         0.5644,\n",
        "                         4.3805,\n",
        "                         ],\n",
        "                 'agnt': [\n",
        "                          0.5464,\n",
        "                          5.8929,\n",
        "                          ]\n",
        "                 }\n",
        "\n",
        "# Iterate over models\n",
        "\n",
        "for model_type, model_name in zip(model_types, model_names):\n",
        "\n",
        "    # Iterate over subconstructs\n",
        "\n",
        "    for target in [\n",
        "                   'refl',\n",
        "                   'just',\n",
        "                   'afrm',\n",
        "                   'fit',\n",
        "                   'agnt',\n",
        "                   ]:\n",
        "\n",
        "        # Stratified train-test split\n",
        "\n",
        "#        train_df, eval_df = train_test_split(\n",
        "#                                             trans_df[['text', target]],\n",
        "#                                             test_size = 0.2,\n",
        "#                                             stratify = trans_df[target],\n",
        "#                                             random_state = 56,\n",
        "#                                             )\n",
        "\n",
        "        # Subconstruct-stratified 5-fold cv\n",
        "\n",
        "        d_subset = d[['text', target, 'augment']]\n",
        "\n",
        "        kf = KFold(\n",
        "                   n_splits = 5,\n",
        "                   shuffle = True,\n",
        "                   random_state = 56,\n",
        "                   )\n",
        "\n",
        "        for train_index, eval_index in kf.split(d_subset):\n",
        "            train_d, eval_d = d_subset.iloc[train_index], d_subset.iloc[eval_index]\n",
        "\n",
        "            eval_d = eval_d[eval_d['augment'] != 1]\n",
        "\n",
        "            train_and_evaluate_model(\n",
        "                                     model_type,\n",
        "                                     model_name,\n",
        "                                     target,\n",
        "                                     class_weights,\n",
        "                                     train_d,\n",
        "                                     eval_d,\n",
        "                                     )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cd1a5fa-b84e-4b09-9fc7-2e89e4ea726c",
      "metadata": {
        "id": "3cd1a5fa-b84e-4b09-9fc7-2e89e4ea726c"
      },
      "source": [
        "> End of mh_audit_v1.ipynb (03-28-2024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e5a95c2-e501-48ee-a307-568c6744cf64",
      "metadata": {
        "id": "2e5a95c2-e501-48ee-a307-568c6744cf64"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}