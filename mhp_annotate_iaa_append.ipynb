{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wm6FpVDfg_2G",
        "RBqI-ti45zRF",
        "3T01CPB1-0Jd",
        "OMaryzhl9FG8",
        "c7mqGlB3hHCc",
        "KiMmqFPphSKE",
        "QBx7DDRP8VRj",
        "z5y8kU5C-FZJ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Linguistic markers of subtle discrimination among mental healthcare professionals"
      ],
      "metadata": {
        "id": "bMLJKSegTg85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_WIP - NOT FOR DISTRIBUTION_\n",
        "\n",
        "_Organizes iterative, independent co-annotation of audit correspondence field experiment responses receievd from mental health professionals. Samples parent study data by co-annotation cycle, computes Cohen's $\\kappa$, flags discrepant tagging decisions for in-person deliberation. Extracts .html profile attribiutes and background vars._"
      ],
      "metadata": {
        "id": "eOlepkBxTjq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> mhp_annotate_iaa_append.ipynb<br>\n",
        "> Simone J. Skeen (10-22-2024)"
      ],
      "metadata": {
        "id": "CMrM5hSKTnBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. [Prepare](#scrollTo=_nwtco4XT0CL)\n",
        "2. [Write](#scrollTo=EavEs0OkbeHT)\n",
        "3. [Sample](#scrollTo=c7mqGlB3hHCc)\n",
        "4. [Triangulate](#scrollTo=z5y8kU5C-FZJ)\n",
        "5. [Extract](#scrollTo=S2aMoYZlA-k3)"
      ],
      "metadata": {
        "id": "beZBzyJcUF5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare\n",
        "Installs, imports, and downloads requisite models and packages. Organizes RAP-consistent directory structure.\n",
        "***"
      ],
      "metadata": {
        "id": "_nwtco4XT0CL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install**"
      ],
      "metadata": {
        "id": "zj38Do4KVDwa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4h_bvnl6M_E_",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "%pip install openai\n",
        "\n",
        "#!python -m spacy download en_core_web_lg --user"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import**"
      ],
      "metadata": {
        "id": "xaIEFfUKVHt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import openai\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "#spacy.cli.download('en_core_web_lg')\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = 'all'\n",
        "\n",
        "pd.options.mode.copy_on_write = True\n",
        "\n",
        "pd.set_option(\n",
        "              'display.max_columns',\n",
        "              None,\n",
        "              )\n",
        "\n",
        "pd.set_option(\n",
        "              'display.max_rows',\n",
        "              None,\n",
        "              )\n",
        "\n",
        "warnings.simplefilter(\n",
        "                      action = 'ignore',\n",
        "                      category = FutureWarning,\n",
        "                      )\n",
        "\n",
        "#!python -m prodigy stats"
      ],
      "metadata": {
        "id": "SmiiZjV4VGn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set env variables**"
      ],
      "metadata": {
        "id": "zT92s0KO9iKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = '<my_key>'\n",
        "os.environ"
      ],
      "metadata": {
        "id": "KY2W6vSy9hbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mount gdrive**"
      ],
      "metadata": {
        "id": "sovUdXg3VQCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\n",
        "            '/content/drive',\n",
        "            #force_remount = True,\n",
        "            )"
      ],
      "metadata": {
        "id": "g-wBTqXnVGrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Structure directories**"
      ],
      "metadata": {
        "id": "KcWzHxh_VokA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Colab/mhp_subtle_discrimination\n",
        "#%cd /content/drive/My Drive/#<my_project_folder>\n",
        "\n",
        "#%mkdir inputs"
      ],
      "metadata": {
        "id": "vwvddFaBVn4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%cd inputs\n",
        "#%mkdir annotation data html"
      ],
      "metadata": {
        "id": "DFgYPyRgWBDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mhp_subtle_discrimination/\n",
        "└── inputs/\n",
        "    ├── annotation/\n",
        "    │   └── #d_cycle_{0...n}xlsx\n",
        "    ├── data/\n",
        "    │   └── d_pilot.xlsx\n",
        "    └── html/"
      ],
      "metadata": {
        "id": "mv8tmIDVWG65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Housekeeping: $\\mathcal{d}$<sub>pilot</sub>"
      ],
      "metadata": {
        "id": "r04phS9CYvja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd inputs/data\n",
        "\n",
        "d_pilot = pd.read_excel(\n",
        "                        'd_pilot.xlsx',\n",
        "                        index_col = 'index',\n",
        "                        )\n",
        "# 'pilot' var\n",
        "\n",
        "d_pilot['pilot'] = 1\n",
        "\n",
        "# 'MHP ID#' var\n",
        "\n",
        "d_pilot['MHP ID#'] = '.'\n",
        "\n",
        "d_pilot.info()\n",
        "d_pilot.head(3)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0vurCFz1XNw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_pilot.to_excel(\n",
        "                 'd_pilot.xlsx',\n",
        "                 index = True,\n",
        "                 )"
      ],
      "metadata": {
        "id": "6FgHFPu5Zrj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Write\n",
        "Writes and imports custom modules"
      ],
      "metadata": {
        "id": "EavEs0OkbeHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd code"
      ],
      "metadata": {
        "id": "5xrQsTRDbMEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### preprocess.py"
      ],
      "metadata": {
        "id": "wm6FpVDfg_2G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**_ner_redact_response_texts_**"
      ],
      "metadata": {
        "id": "IPVS89cucBdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile preprocess.py\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "def ner_redact_response_texts(mhp_text):\n",
        "    \"\"\"\n",
        "    Redacts all named entities recognized by spaCy EntityRecognizer, replaces with <|PII|> pseudo-word token.\n",
        "    \"\"\"\n",
        "    ne = list(\n",
        "              [\n",
        "               'PERSON',   ### people, including fictional\n",
        "               'NORP',     ### nationalities or religious or political groups\n",
        "               'FAC',      ### buildings, airports, highways, bridges, etc.\n",
        "               'ORG',      ### companies, agencies, institutions, etc.\n",
        "               'GPE',      ### countries, cities, states\n",
        "               'LOC',      ### non-GPE locations, mountain ranges, bodies of water\n",
        "               'PRODUCT',  ### objects, vehicles, foods, etc. (not services)\n",
        "               'EVENT',    ### named hurricanes, battles, wars, sports events, etc.\n",
        "               ]\n",
        "                )\n",
        "\n",
        "    doc = nlp(mhp_text)\n",
        "    ne_to_remove = []\n",
        "    final_string = str(mhp_text)\n",
        "    for sent in doc.ents:\n",
        "        if sent.label_ in ne:\n",
        "            ne_to_remove.append(str(sent.text))\n",
        "    for i in range(len(ne_to_remove)):\n",
        "        final_string = final_string.replace(\n",
        "                                            ne_to_remove[i],\n",
        "                                            '<|PII|>',\n",
        "                                            )\n",
        "    return final_string"
      ],
      "metadata": {
        "id": "tM2vQMqwb13F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### annotate.py"
      ],
      "metadata": {
        "id": "RBqI-ti45zRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**_sample_by_cycle_**"
      ],
      "metadata": {
        "id": "jST11rvZpiOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile annotate.py\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def sample_by_cycle(d_pilot, sample_size, cycle_number):\n",
        "    \"\"\"\n",
        "    Creates random subsample of d_pilot, excises prior tags, and unneeded columns,\n",
        "    exports to .xlsx for human annotation.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "\n",
        "    d_pilot : pd.DataFrame\n",
        "        The d_pilot df in memory.\n",
        "\n",
        "    sample_size : int\n",
        "        The number of rows to sample from d_pilot.\n",
        "\n",
        "    cycle_number : int\n",
        "        The cycle number used to name the output dataframe and the Excel file.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        A new dataframe called d_cycle_{cycle_number} after applying the operations.\n",
        "    \"\"\"\n",
        "\n",
        "    d_cycle = d_pilot.sample(\n",
        "                             n = sample_size,\n",
        "                             #random_state = 56,\n",
        "                             )\n",
        "\n",
        "    # reset index\n",
        "\n",
        "    d_cycle.reset_index(\n",
        "                        drop = True,\n",
        "                        inplace = True,\n",
        "                        )\n",
        "    # DAL themes\n",
        "\n",
        "    d_cycle['brdn'] = ' '\n",
        "    d_cycle['dmnd'] = ' '\n",
        "    d_cycle['rbnd'] = ' '\n",
        "\n",
        "    # excise prior tags\n",
        "\n",
        "    tag_cols = [\n",
        "                'prbl',\n",
        "                'refl',\n",
        "                'just',\n",
        "                'afrm',\n",
        "                'fitt',\n",
        "                'agnt',\n",
        "                'brdn',\n",
        "                'dmnd',\n",
        "                'rbnd',\n",
        "                'rtnl',\n",
        "                'note',\n",
        "                ]\n",
        "\n",
        "    d_cycle[tag_cols] = ' '\n",
        "\n",
        "    # excise unneeded columns\n",
        "\n",
        "    drop_cols = [\n",
        "                 'EmailPairID',\n",
        "                 'WithinPatientID',\n",
        "                 'FirstInPair',\n",
        "                 'pilot',\n",
        "                 'MHP ID#',\n",
        "                 ]\n",
        "\n",
        "    d_cycle.drop(\n",
        "                 columns = drop_cols,\n",
        "                 axis = 1,\n",
        "                 inplace = True,\n",
        "                 )\n",
        "\n",
        "    # export\n",
        "\n",
        "    filename = f'd_cycle_{cycle_number}.xlsx'\n",
        "\n",
        "    d_cycle.to_excel(\n",
        "                     filename,\n",
        "                     index = True,\n",
        "                     )\n",
        "\n",
        "    return d_cycle"
      ],
      "metadata": {
        "id": "NQ4qkUD5hF0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### calculate.py"
      ],
      "metadata": {
        "id": "3T01CPB1-0Jd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**_calculate_kappa_by_cycle_**"
      ],
      "metadata": {
        "id": "QcaU3n01-9HQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile calculate.py\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "def calculate_kappa_by_cycle(cycle_num):\n",
        "    \"\"\"\n",
        "    Calculate Cohen's Kappa and encode disagreements between independent annotators across multiple cycles.\n",
        "\n",
        "    Parameters:\n",
        "    --------\n",
        "    cycle_num : int\n",
        "        Annotation cycle number, used to load the corresponding Excel files (e.g., cycle 0, cycle 1).\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    d : pd.DataFrame\n",
        "        Processed df after merging, includes encoded disagreements in *_dis columns.\n",
        "\n",
        "    kappa_results : dict\n",
        "        A dictionary containing the Cohen's Kappa scores for each indepednently co-annotated target.\n",
        "    \"\"\"\n",
        "    # read independently annotated files\n",
        "\n",
        "    d_dal = pd.read_excel(f'd_cycle_{cycle_num}_dal.xlsx', index_col = [0])\n",
        "    d_dal.columns = [f'{col}_dal' for col in d_dal.columns]\n",
        "\n",
        "    d_sjs = pd.read_excel(f'd_cycle_{cycle_num}_sjs.xlsx', index_col = [0])\n",
        "    d_sjs.columns = [f'{col}_sjs' for col in d_sjs.columns]\n",
        "\n",
        "    # merge\n",
        "\n",
        "    d = pd.merge(\n",
        "                 d_dal,\n",
        "                 d_sjs,\n",
        "                 left_index = True,\n",
        "                 right_index = True,\n",
        "                 )\n",
        "\n",
        "    # housekeeping\n",
        "\n",
        "    targets = [\n",
        "               'afrm_dal', 'afrm_sjs',\n",
        "               'agnt_dal', 'agnt_sjs',\n",
        "               'brdn_dal', 'brdn_sjs',\n",
        "               'dmnd_dal', 'dmnd_sjs',\n",
        "               'fitt_dal', 'fitt_sjs',\n",
        "               'just_dal', 'just_sjs',\n",
        "               'prbl_dal', 'prbl_sjs',\n",
        "               'rbnd_dal', 'rbnd_sjs',\n",
        "               'refl_dal', 'refl_sjs',\n",
        "               ]\n",
        "\n",
        "    texts = [\n",
        "             'text_dal', 'text_sjs',\n",
        "             'rtnl_dal', 'rtnl_sjs',\n",
        "             'note_dal', 'note_sjs',\n",
        "             ]\n",
        "\n",
        "    d[targets] = d[targets].apply(\n",
        "                                  pd.to_numeric,\n",
        "                                  errors = 'coerce',\n",
        "                                  )\n",
        "    d[targets] = d[targets].fillna(0)\n",
        "    d[texts] = d[texts].replace(' ', '.')\n",
        "\n",
        "    d = d[[\n",
        "           'text_dal',\n",
        "           'afrm_dal', 'afrm_sjs',\n",
        "           'agnt_dal', 'agnt_sjs',\n",
        "           'brdn_dal', 'brdn_sjs',\n",
        "           'dmnd_dal', 'dmnd_sjs',\n",
        "           'fitt_dal', 'fitt_sjs',\n",
        "           'just_dal', 'just_sjs',\n",
        "           'prbl_dal', 'prbl_sjs',\n",
        "           'rbnd_dal', 'rbnd_sjs',\n",
        "           'refl_dal', 'refl_sjs',\n",
        "           'rtnl_dal', 'rtnl_sjs',\n",
        "           'note_dal', 'note_sjs',\n",
        "           ]].copy()\n",
        "\n",
        "    # kappa Fx\n",
        "\n",
        "    def calculate_kappa(d, col_dal, col_sjs):\n",
        "        return cohen_kappa_score(d[col_dal], d[col_sjs])\n",
        "\n",
        "    col_pairs = [\n",
        "                 ('afrm_dal', 'afrm_sjs'),\n",
        "                 ('agnt_dal', 'agnt_sjs'),\n",
        "                 ('brdn_dal', 'brdn_sjs'),\n",
        "                 ('dmnd_dal', 'dmnd_sjs'),\n",
        "                 ('fitt_dal', 'fitt_sjs'),\n",
        "                 ('just_dal', 'just_sjs'),\n",
        "                 ('prbl_dal', 'prbl_sjs'),\n",
        "                 ('rbnd_dal', 'rbnd_sjs'),\n",
        "                 ('refl_dal', 'refl_sjs'),\n",
        "                 ]\n",
        "\n",
        "    # initialize dict\n",
        "\n",
        "    kappa_results = {}\n",
        "\n",
        "    # kappa loop\n",
        "    print(\"\\n--------------------------------------------------------------------------------------\")\n",
        "    print(f\"Cycle {cycle_num}: Cohen's Kappa by target\")\n",
        "    print(\"--------------------------------------------------------------------------------------\")\n",
        "\n",
        "    for col_dal, col_sjs in col_pairs:\n",
        "        kappa = calculate_kappa(d, col_dal, col_sjs)\n",
        "        kappa_results[f'{col_dal} and {col_sjs}'] = kappa\n",
        "\n",
        "    for pair, kappa in kappa_results.items():\n",
        "        print(f\"{pair} Kappa = {kappa:.2f}\")\n",
        "\n",
        "    # dummy code disagreements Fx\n",
        "\n",
        "    def encode_disagreements(row):\n",
        "        return 1 if row[0] != row[1] else 0\n",
        "\n",
        "    col_dis = [\n",
        "               ('afrm_dal', 'afrm_sjs', 'afrm_dis'),\n",
        "               ('agnt_dal', 'agnt_sjs', 'agnt_dis'),\n",
        "               ('brdn_dal', 'brdn_sjs', 'brdn_dis'),\n",
        "               ('dmnd_dal', 'dmnd_sjs', 'dmnd_dis'),\n",
        "               ('fitt_dal', 'fitt_sjs', 'fitt_dis'),\n",
        "               ('just_dal', 'just_sjs', 'just_dis'),\n",
        "               ('prbl_dal', 'prbl_sjs', 'prbl_dis'),\n",
        "               ('rbnd_dal', 'rbnd_sjs', 'rbnd_dis'),\n",
        "               ('refl_dal', 'refl_sjs', 'refl_dis'),\n",
        "               ]\n",
        "\n",
        "    for col1, col2, dis_col in col_dis:\n",
        "        d[dis_col] = d[[col1, col2]].apply(encode_disagreements, axis = 1)\n",
        "\n",
        "    # display counts for targets\n",
        "\n",
        "    print(\"\\n--------------------------------------------------------------------------------------\")\n",
        "    print(f\"Cycle {cycle_num}: Counts by target\")\n",
        "    print(\"--------------------------------------------------------------------------------------\")\n",
        "    print(d[targets].apply(pd.Series.value_counts))\n",
        "\n",
        "    # drop target cols for readability + fillna\n",
        "\n",
        "    d = d.drop(targets, axis = 1)\n",
        "    d = d.fillna('.')\n",
        "\n",
        "    # export: cycle-specific\n",
        "\n",
        "    d.to_excel(f'd_cycle_{cycle_num}_dis.xlsx')\n",
        "\n",
        "    return d, kappa_results"
      ],
      "metadata": {
        "id": "sF36H1rAkEd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### gpt_assist.py"
      ],
      "metadata": {
        "id": "OMaryzhl9FG8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**_transform_text_with_gpt_**"
      ],
      "metadata": {
        "id": "u3HqZNrw89M0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile gpt_assist.py\n",
        "\n",
        "import pandas as pd\n",
        "import openai\n",
        "import time\n",
        "\n",
        "def transform_text_with_gpt(df, input_column, output_column, system_prompt, prompt_template, model='gpt-4o'):\n",
        "    \"\"\"\n",
        "    Transforms text data in a specified df column using GPT based on provided prompts.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): df containing the text to be transformed.\n",
        "        input_column (str): name of the input column in the df that contains the text to transform.\n",
        "        output_column (str): name of the output column where the transformed text will be stored.\n",
        "        system_prompt (str): system prompt that sets up the assistant's behavior.\n",
        "        prompt_template (str): template string describing the transformation to be applied to each entry.\n",
        "          Use '{input_text}' as a placeholder for the input text.\n",
        "        model (str, optional): The name of the OpenAI GPT model to use (default = 'gpt-4o').\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: df with new output column added, containing the transformed text.\n",
        "    \"\"\"\n",
        "\n",
        "    # Fx to send row-wise API requests\n",
        "\n",
        "    def call_gpt(input_text):\n",
        "        if pd.isnull(input_text) or input_text.strip() == ' ':\n",
        "            return ' '\n",
        "\n",
        "        prompt = prompt_template.format(input_text = input_text)\n",
        "\n",
        "        try:\n",
        "            response = openai.chat.completions.create(\n",
        "                                                      model = model,\n",
        "                                                      messages = [\n",
        "                                                                  {'role': 'system',\n",
        "                                                                   'content': system_prompt},\n",
        "                                                                  {'role': 'user',\n",
        "                                                                   'content': prompt},\n",
        "                                                                  ],\n",
        "                                                      #max_tokens = 500,\n",
        "                                                      #n = 1,\n",
        "                                                      #temperature = 0,\n",
        "                                                      )\n",
        "\n",
        "            # extract text from API response\n",
        "\n",
        "            result = response.choices[0].message.content.strip()\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing input text: {input_text}\\nError: {str(e)}\")\n",
        "            return input_text ### returns input string in case of error\n",
        "\n",
        "        finally:\n",
        "\n",
        "            # impose delay between API calls\n",
        "\n",
        "            time.sleep(1)\n",
        "\n",
        "    df[output_column] = df[input_column].apply(call_gpt)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "XU_sPaVr85V5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import"
      ],
      "metadata": {
        "id": "jHYGcDchhvXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from annotate import(\n",
        "                     sample_by_cycle\n",
        "                     )\n",
        "\n",
        "#from preprocess import(\n",
        "#                       ner_redact_response_texts\n",
        "#                       )\n",
        "\n",
        "from calculate import(\n",
        "                      calculate_kappa_by_cycle\n",
        "                      )\n",
        "\n",
        "from gpt_assist import(\n",
        "                       transform_text_with_gpt\n",
        "                       )"
      ],
      "metadata": {
        "id": "9_gxgX1whuYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Sample\n",
        "Randomly samples cycle-specific MHP response subsets for annotation.\n",
        "***"
      ],
      "metadata": {
        "id": "c7mqGlB3hHCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "id": "BZpN1kTe6LKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../inputs/data\n",
        "\n",
        "d_pilot = pd.read_excel(\n",
        "                        'd_pilot.xlsx',\n",
        "                        index_col = 'index',\n",
        "                        )\n",
        "\n",
        "#d_pilot.info()\n",
        "#d_pilot.head(3)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2Qk4fvgs6AE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cycle 0"
      ],
      "metadata": {
        "id": "KiMmqFPphSKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../annotation"
      ],
      "metadata": {
        "id": "8sgZQ-aejGQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample\n",
        "\n",
        "d_cycle_0 = d_pilot.sample(\n",
        "                           n = 50,\n",
        "                           random_state = 56,\n",
        "                           )\n",
        "\n",
        "# reset index\n",
        "\n",
        "d_cycle_0.reset_index(\n",
        "                      drop = True,\n",
        "                      inplace = True,\n",
        "                      )\n",
        "\n",
        "# excise prior tags\n",
        "\n",
        "tag_cols = [\n",
        "            'afrm',\n",
        "            'agnt',\n",
        "            'fitt',\n",
        "            'just',\n",
        "            'prbl',\n",
        "            'refl',\n",
        "            'rtnl',\n",
        "            'note',\n",
        "            ]\n",
        "\n",
        "d_cycle_0[tag_cols] = ' '\n",
        "\n",
        "# excise unneeded cols\n",
        "\n",
        "drop_cols = [\n",
        "             'EmailPairID',\n",
        "             'WithinPatientID',\n",
        "             'FirstInPair',\n",
        "             'pilot',\n",
        "             'MHP ID#',\n",
        "             ]\n",
        "\n",
        "    ### SJS 9/16: add DAL targets (for now): brdn, dmnd, rbnd\n",
        "\n",
        "d_cycle_0.drop(\n",
        "               columns = drop_cols,\n",
        "               axis = 1,\n",
        "               inplace = True,\n",
        "               )\n",
        "\n",
        "# export\n",
        "\n",
        "d_cycle_0.head(3)\n",
        "\n",
        "d_cycle_0.to_excel(\n",
        "                   'd_cycle_0.xlsx',\n",
        "                   index = True,\n",
        "                   )"
      ],
      "metadata": {
        "id": "kpbCJGH7hF9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cycle 1"
      ],
      "metadata": {
        "id": "QBx7DDRP8VRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../annotation"
      ],
      "metadata": {
        "id": "E4FBQNvf8Ua7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# call sample_by_cycle\n",
        "\n",
        "d_cycle_1 = sample_by_cycle(\n",
        "                            d_pilot,\n",
        "                            80, # sample_size = 80\n",
        "                            1, # cycle_number = 1\n",
        "                            )\n",
        "\n",
        "d_cycle_1.info()\n",
        "d_cycle_1.head(3)"
      ],
      "metadata": {
        "id": "_JvstXci8knC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cycle 2"
      ],
      "metadata": {
        "id": "kUdQuLgg4tkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../annotation"
      ],
      "metadata": {
        "id": "V5MbN2sKy61c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# call sample_by_cycle\n",
        "\n",
        "d_cycle_2 = sample_by_cycle(\n",
        "                            d_pilot,\n",
        "                            80, # sample_size = 80\n",
        "                            2, # cycle_number = 2\n",
        "                            )\n",
        "\n",
        "d_cycle_2.info()\n",
        "d_cycle_2.head(3)"
      ],
      "metadata": {
        "id": "fV9f1gSSy69g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Triangulate\n",
        "Computes Cohen's $\\kappa$, dummy codes discrepant tags for in-person deliberation.\n",
        "***"
      ],
      "metadata": {
        "id": "z5y8kU5C-FZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cycle 0"
      ],
      "metadata": {
        "id": "ulOY0VaRPTbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../inputs/annotation\n",
        "\n",
        "d, kappa_results = calculate_kappa_by_cycle(0)"
      ],
      "metadata": {
        "id": "c3x-67MthGCK",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cycle 1"
      ],
      "metadata": {
        "id": "OPdsTqdcPe-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../inputs/annotation\n",
        "\n",
        "d, kappa_results = calculate_kappa_by_cycle(1)"
      ],
      "metadata": {
        "id": "xKTKPN7HA9qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d.head(3)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1bjgMdgqS8Uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Extract\n",
        "Uses substring extraction, regex, and GPT-4 API to restructure .htm and .html into MHP-indexed df of background attributes\n",
        "***"
      ],
      "metadata": {
        "id": "S2aMoYZlA-k3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Colab/mhp_subtle_discrimination/inputs/html\n",
        "#del d"
      ],
      "metadata": {
        "id": "ddsBlv8AHqvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../inputs/html\n",
        "\n",
        "html = [file for file in os.listdir() if file.endswith(('.htm', '.html'))]\n",
        "\n",
        "# initialize list\n",
        "\n",
        "html_data = []\n",
        "\n",
        "# load\n",
        "\n",
        "for h in html:\n",
        "    with open(h, 'r', encoding = 'utf-8') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    # parse .html\n",
        "\n",
        "    soup = BeautifulSoup(\n",
        "                         content,\n",
        "                         'html.parser',\n",
        "                         )\n",
        "\n",
        "    # extract attributes\n",
        "\n",
        "    name_title = soup.find(\n",
        "                           'meta',\n",
        "                           property = 'og:title',\n",
        "                           )\n",
        "\n",
        "    profile = soup.find(\n",
        "                        'meta',\n",
        "                        property = 'og:url',\n",
        "                        )\n",
        "    image = soup.find(\n",
        "                      'meta',\n",
        "                      property = 'og:image',\n",
        "                      )\n",
        "\n",
        "    image_alt = soup.find(\n",
        "                          'meta',\n",
        "                          property = 'og:image:alt',\n",
        "                          )\n",
        "\n",
        "    place = soup.find(\n",
        "                      'meta',\n",
        "                      attrs = {'name': 'geo.placename'},\n",
        "                      )\n",
        "\n",
        "    # extract attribute contents\n",
        "\n",
        "    practice_name_text = name_title['content'] if name_title else '.'\n",
        "    profile_url = profile['content'] if profile else '.'\n",
        "    image_url = image['content'] if image else '.'\n",
        "    image_alt_text = image_alt['content'] if image_alt else '.'\n",
        "    place_name = place['content'] if place else '.'\n",
        "\n",
        "    # extract filename as MHP ID\n",
        "\n",
        "    mhp_id = h.replace('.html', ' ').replace('.htm', ' ')\n",
        "\n",
        "    # extract full text\n",
        "\n",
        "    full_text = soup.get_text()\n",
        "\n",
        "    # 'pronouns' str: extract text preceding \"Verified\"\n",
        "\n",
        "    extracted_text = re.search(\n",
        "                               r'^(.*?)Verified',\n",
        "                               full_text,\n",
        "                               re.DOTALL,\n",
        "                               )\n",
        "\n",
        "    if extracted_text:\n",
        "        extracted_text = extracted_text.group(1).strip()\n",
        "    else:\n",
        "        extracted_text = ' '\n",
        "\n",
        "    # extract pronouns from parens\n",
        "\n",
        "    pronoun_text = re.findall(r'\\(([^0-9]+?)\\)', extracted_text)\n",
        "    pronoun_text = ' '.join(pronoun_text).strip()\n",
        "\n",
        "    # 'description' str: extract text between \"Let's Connect\" and \"Call or Email\"\n",
        "\n",
        "    start_description = full_text.find(\"Let's Connect\")\n",
        "    end_description = full_text.find(\"Call or Email\", start_description)\n",
        "    description_text = full_text[start_description + len(\"Let's Connect\"):end_description].strip() \\\n",
        "    if start_description != -1 \\\n",
        "    and end_description != -1 \\\n",
        "    else '.'\n",
        "\n",
        "    # 'at_a_glance' str (incl 'finances'): extract text between \"Practice at a Glance\" and \"Qualifications\"\n",
        "\n",
        "    start_glance = full_text.find(\"Practice at a Glance\")\n",
        "    end_glance = full_text.find(\"Qualifications\", start_glance)\n",
        "    glance_text = full_text[start_glance + len(\"Practice at a Glance\"):end_glance].strip() \\\n",
        "    if start_glance != -1 \\\n",
        "    and end_glance != -1 \\\n",
        "    else '.'\n",
        "\n",
        "    # 'qualifications' str: extract text between \"Qualifications\" and \"Feel free to ask\"\n",
        "\n",
        "    start_qualifications = full_text.find(\"Qualifications\")\n",
        "    end_qualifications = full_text.find(\"Feel free to ask\", start_qualifications)\n",
        "    qualifications_text = full_text[start_qualifications + len(\"Qualifications\"):end_qualifications].strip() \\\n",
        "    if start_qualifications != -1 \\\n",
        "    and end_qualifications != -1 \\\n",
        "    else '.'\n",
        "\n",
        "    # 'specialities' str: extract text between \"Top Specialties\" and \"Do these issues\"\n",
        "\n",
        "    start_specialties = full_text.find(\"Top Specialties\")\n",
        "    end_specialties = full_text.find(\"Do these issues\", start_specialties)\n",
        "    specialties_text = full_text[start_specialties + len(\"Top Specialties\"):end_specialties].strip() \\\n",
        "    if start_specialties != -1 \\\n",
        "    and end_specialties != -1 \\\n",
        "    else '.'\n",
        "\n",
        "    # 'client_focus' str: extract text between \"Client Focus\" and \"Treatment Approach\"\n",
        "\n",
        "    start_client = full_text.find(\"Client Focus\")\n",
        "    end_client = full_text.find(\"Treatment Approach\", start_client)\n",
        "    client_text = full_text[start_client + len(\"Client Focus\"):end_client].strip() \\\n",
        "    if start_client != -1 \\\n",
        "    and end_client != -1 \\\n",
        "    else '.'\n",
        "\n",
        "    # 'types_of_therapy' str: extract text between \"Types of Therapy\" and \"Ask about what\"\n",
        "\n",
        "    start_therapy = full_text.find(\"Types of Therapy\")\n",
        "    end_therapy = full_text.find(\"Ask about what\", start_therapy)\n",
        "    therapy_text = full_text[start_therapy + len(\"Types of Therapy\"):end_therapy].strip() \\\n",
        "    if start_therapy != -1 \\\n",
        "    and end_therapy != -1 \\\n",
        "    else '.'\n",
        "\n",
        "    therapy_text_with_commas = re.sub(r'(?<=[a-z])(?=[A-Z])', ', ', therapy_text)\n",
        "    therapy_text_with_commas = re.sub(r'(?<=\\))', ', ', therapy_text_with_commas)\n",
        "\n",
        "    # append to list\n",
        "\n",
        "    html_data.append({\n",
        "                      'MHP ID': mhp_id,\n",
        "                      'practice_name': practice_name_text,\n",
        "                      'pronouns': pronoun_text,\n",
        "                      'description': description_text,\n",
        "                      'profile_url': profile_url,\n",
        "                      'image_url': image_url,\n",
        "                      'image_alt_text': image_alt_text,\n",
        "                      'at_a_glance': glance_text,\n",
        "                      'qualifications': qualifications_text,\n",
        "                      'specialties_raw': specialties_text,\n",
        "                      'client_focus': client_text,\n",
        "                      'types_of_therapy': therapy_text_with_commas,\n",
        "                      'place_name': place_name,\n",
        "                      })\n",
        "\n",
        "# build df\n",
        "\n",
        "d = pd.DataFrame(html_data)\n",
        "\n",
        "# 'client_focus' clean + parse\n",
        "\n",
        "d['client_focus'] = d['client_focus'].str.replace(\n",
        "                                                  r'\\s+,',\n",
        "                                                  ',',\n",
        "                                                  regex = True,\n",
        "                                                  )\n",
        "\n",
        "# 'ages' str: extract text following \"Age\" from 'client_focus'\n",
        "\n",
        "d['ages'] = d['client_focus'].str.extract(\n",
        "                                          r'Age\\s*(.*?)\\s*Participants',\n",
        "                                          flags = re.I,\n",
        "                                          )\n",
        "\n",
        "d['client_focus'] = d['client_focus'].str.replace(\n",
        "                                                  'Age',\n",
        "                                                  ' ',\n",
        "                                                  flags = re.I,\n",
        "                                                  regex = True,\n",
        "                                                  )\n",
        "\n",
        "\n",
        "# 'participants' str: extract text following \"Participants\" from 'client_focus'\n",
        "\n",
        "d['participants'] = d['client_focus'].str.extract(\n",
        "                                                  r'Participants\\s*(.*?)\\s*Communities',\n",
        "                                                  flags = re.I,\n",
        "                                                  )\n",
        "\n",
        "d['client_focus'] = d['client_focus'].str.replace(\n",
        "                                                  'Participants',\n",
        "                                                  ' ',\n",
        "                                                  flags = re.I,\n",
        "                                                  regex = True,\n",
        "                                                  )\n",
        "\n",
        "# 'communities' str: extract text following \"Communities\" from 'client_focus'\n",
        "\n",
        "d['communities'] = d['client_focus'].str.extract(\n",
        "                                                 r'Communities\\s*(.*?)\\s*Ethnicity',\n",
        "                                                 flags = re.I,\n",
        "                                                 )\n",
        "\n",
        "d['client_focus'] = d['client_focus'].str.replace(\n",
        "                                                  'Communities',\n",
        "                                                  ' ',\n",
        "                                                  flags = re.I,\n",
        "                                                  regex = True,\n",
        "                                                  )\n",
        "\n",
        "# 'ethnicities' str: extract text following \"Ethnicity\" from 'client_focus'\n",
        "\n",
        "d['ethnicities'] = d['client_focus'].str.extract(\n",
        "                                                 r'Ethnicity\\s*(.*?)\\s*Religion',\n",
        "                                                 flags = re.I,\n",
        "                                                 )\n",
        "\n",
        "d['client_focus'] = d['client_focus'].str.replace(\n",
        "                                                  'Ethnicity',\n",
        "                                                  ' ',\n",
        "                                                  flags = re.I,\n",
        "                                                  regex = True,\n",
        "                                                  )\n",
        "\n",
        "\n",
        "# 'religions' str: extract text following \"Religion\" from 'client_focus'\n",
        "\n",
        "d['religions'] = d['client_focus'].str.extract(\n",
        "                                              r'(Religion.*)',\n",
        "                                              flags = re.I,\n",
        "                                              expand = False,\n",
        "                                              )\n",
        "\n",
        "d['client_focus'] = d['client_focus'].str.replace(\n",
        "                                                  r'Religion',\n",
        "                                                  ' ',\n",
        "                                                  regex = True,\n",
        "                                                  )\n",
        "\n",
        "d['religions'] = d['religions'].str.replace(\n",
        "                                            r'Religion',\n",
        "                                            ' ',\n",
        "                                            regex = True,\n",
        "                                            )\n",
        "\n",
        "# 'finances' str: extract text following \"Finances\" from 'at_a_glance'\n",
        "\n",
        "d['finances'] = d['at_a_glance'].str.extract(\n",
        "                                             r'Finances(.*)',\n",
        "                                             expand = False,\n",
        "                                             )\n",
        "\n",
        "d['finances'] = d['finances'].fillna('.').str.strip()\n",
        "\n",
        "# delete \"Finances\" from 'at_a_glance'\n",
        "\n",
        "d['at_a_glance'] = d['at_a_glance'].str.replace(\n",
        "                                                r'Finances.*',\n",
        "                                                ' ',\n",
        "                                                regex = True,\n",
        "                                                n = 1,\n",
        "                                                ).str.strip()\n",
        "\n",
        "# 'name' str: extract from 'practice_name'\n",
        "\n",
        "d['name'] = d['practice_name']\n",
        "d['name'] = d['name'].str.split(\n",
        "                                ',',\n",
        "                                n = 1,\n",
        "                                ).str[0].str.strip()\n",
        "\n",
        "# 'availability' str: extract (pre-specified) from 'at_a_glance'\n",
        "\n",
        "availabilities = [\n",
        "                  'Available both in-person and online',\n",
        "                  'Available in-person',\n",
        "                  'Available online',\n",
        "                  ]\n",
        "\n",
        "d['availability'] = d['at_a_glance'].str.extract(\n",
        "                                                 f\"({'|'.join(availabilities)})\",\n",
        "                                                 expand = False,\n",
        "                                                 )\n",
        "\n",
        "d['availability'] = d['availability'].fillna('.')\n",
        "\n",
        "# 'years_in_practice' str: extract from 'qualifications\n",
        "\n",
        "d['years_in_practice'] = d['qualifications'].str.extract(\n",
        "                                                         r'In Practice for (\\d+) Years',\n",
        "                                                         expand = False,\n",
        "                                                         )\n",
        "\n",
        "d['years_in_practice'] = pd.to_numeric(\n",
        "                                       d['years_in_practice'],\n",
        "                                       errors = 'coerce',\n",
        "                                       )\n",
        "\n",
        "d['years_in_practice'] = d['years_in_practice'].fillna('.')\n",
        "\n",
        "# 'licensed_by_state' str: extract from 'qualifications\n",
        "\n",
        "d['licensed_by_state'] = d['qualifications'].str.extract(r'Licensed by State of ([A-Za-z]+(?: [A-Za-z]+)*)')\n",
        "\n",
        "# 'license_number' int: extract after \"State /\"\n",
        "\n",
        "d['license_number'] = d['qualifications'].str.extract(r'/\\s*(\\d+)')\n",
        "\n",
        "# 'insurance_raw' str: extract from 'finances' - GPT-4o to polish\n",
        "\n",
        "d['insurance_raw'] = d['finances'].str.extract(r'Insurance\\s*(.*)')\n",
        "d['insurance_raw'] = d['insurance_raw'].str.replace(\n",
        "                                                    r'Check fees.*',\n",
        "                                                    ' ',\n",
        "                                                    regex = True,\n",
        "                                                    )\n",
        "\n",
        "# 'fees' str: extract from 'finances'\n",
        "\n",
        "d['fees'] = d['finances'].str.extract(r'Fees\\s*(.*?)\\s*Insurance')\n",
        "\n",
        "# 'individual_fee' int: extract from 'fees'\n",
        "\n",
        "d['individual_fee'] = d['fees'].str.extract(r'Individual Sessions\\s*\\$?(\\d+)')\n",
        "\n",
        "# 'couple_fee' int: extract from 'fees'\n",
        "\n",
        "d['couple_fee'] = d['fees'].str.extract(r'Couple Sessions\\s*\\$?(\\d+)')\n",
        "d['couple_fee'].fillna(\n",
        "                       '.',\n",
        "                       inplace = True,\n",
        "                       )\n",
        "\n",
        "# 'sliding_scale' bool: extract from 'fee's\n",
        "\n",
        "d['sliding_scale'] = d['fees'].str.contains(\n",
        "                                            'Sliding scale:',\n",
        "                                            regex = False,\n",
        "                                            ).fillna(False).astype(int)\n",
        "\n",
        "# delete PT footer from 'practice_name'\n",
        "\n",
        "d['practice_name'] = d['practice_name'].str.replace(\n",
        "                                                    '| Psychology Today',\n",
        "                                                    ' ',\n",
        "                                                    regex = False,\n",
        "                                                    )\n",
        "\n",
        "# delete contact details from 'description'\n",
        "\n",
        "tel_re = r'\\(\\d{3}\\) \\d{3}-\\d{4}'\n",
        "\n",
        "d['description'] = d['description'].str.replace(\n",
        "                                                'Take the first step to help',\n",
        "                                                ' ',\n",
        "                                                regex = False,\n",
        "                                                )\n",
        "\n",
        "d['description'] = d['description'].str.replace(\n",
        "                                                'Email me',\n",
        "                                                ' ',\n",
        "                                                regex = False,\n",
        "                                                )\n",
        "\n",
        "d['description'] = d['description'].str.replace(\n",
        "                                                'Email us',\n",
        "                                                ' ',\n",
        "                                                regex = False,\n",
        "                                                )\n",
        "\n",
        "d['description'] = d['description'].str.replace(\n",
        "                                                tel_re,\n",
        "                                                ' ',\n",
        "                                                regex = True,\n",
        "                                                )\n",
        "\n",
        "# excise artifacts, 'description'\n",
        "\n",
        "artifact_re = '^\\s*[xX]\\d+\\s*'\n",
        "\n",
        "d['description'] = d['description'].str.replace(\n",
        "                                                artifact_re,\n",
        "                                                ' ',\n",
        "                                                regex = True,\n",
        "                                                )\n",
        "\n",
        "d['description'] = d['description'].str.replace(\n",
        "                                                '\\n',\n",
        "                                                ' ',\n",
        "                                                regex = True,\n",
        "                                                )\n",
        "\n",
        "# excise leading, excess spaces, 'description'\n",
        "\n",
        "d['description'] = d['description'].str.strip().str.replace(\n",
        "                                                            '\\s+',\n",
        "                                                            ' ',\n",
        "                                                            regex = True,\n",
        "                                                            )\n",
        "\n",
        "# delete duped text (follows \"Let's Connect\") from 'at_a_glance'\n",
        "\n",
        "d['at_a_glance'] = d['at_a_glance'].str.replace(\n",
        "                                                r\"Let's Connect.*\",\n",
        "                                                ' ',\n",
        "                                                regex = True,\n",
        "                                                ).str.strip()\n",
        "\n",
        "# add space: 'specialties_raw'\n",
        "\n",
        "d['specialties_raw'] = d['specialties_raw'].str.replace(\n",
        "                                                        r'([a-z])([A-Z])',\n",
        "                                                        r'\\1 \\2',\n",
        "                                                        regex = True,\n",
        "                                                        )\n",
        "\n",
        "d['specialties_raw'] = d['specialties_raw'].str.replace(\n",
        "                                                        r'(\\(BPD\\)|OCD\\)|ADHD|LGBTQ\\+|PTSD)',\n",
        "                                                        r'\\1 ',\n",
        "                                                        regex = True,\n",
        "                                                        )\n",
        "\n",
        "d['specialties_raw'] = d['specialties_raw'].str.strip()\n",
        "\n",
        "# delete whitespace from 'pronouns'\n",
        "\n",
        "d['pronouns'] = d['pronouns'].replace(\n",
        "                                      ' ',\n",
        "                                      np.nan,\n",
        "                                      ).fillna('.').str.strip()\n",
        "\n",
        "d['pronouns'] = d['pronouns'].replace(\n",
        "                                      r'^\\s*$',\n",
        "                                      '.',\n",
        "                                      regex = True,\n",
        "                                      )\n",
        "\n",
        "# replace NaN, empty cells, with \".\"\n",
        "\n",
        "d.fillna(\n",
        "         '.',\n",
        "         inplace = True,\n",
        "         )\n",
        "\n",
        "d.replace(\n",
        "          r'^\\s*$', '.',\n",
        "          regex = True,\n",
        "          inplace = True,\n",
        "          )\n",
        "\n",
        "# inspect\n",
        "\n",
        "#d\n",
        "#d.info()\n",
        "#d.head(3)"
      ],
      "metadata": {
        "id": "sUSU2khWXnoq",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dummy code populations**"
      ],
      "metadata": {
        "id": "-HGCaoYyHqHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define Fx: convert str to snake_case\n",
        "\n",
        "def to_snake_case(term):\n",
        "    return term.strip().replace(\" \", \"_\").lower()\n",
        "\n",
        "# split 'client_focus' populations by commas, explode the lists into separate rows\n",
        "\n",
        "d['ages_split'] = d['ages'].str.split(\",\")\n",
        "d['communities_split'] = d['communities'].str.split(\",\")\n",
        "d['ethnicities_split'] = d['ethnicities'].str.split(\",\")\n",
        "d['religions_split'] = d['religions'].str.split(\",\")\n",
        "\n",
        "# define all unique populations (converted to snake case)\n",
        "\n",
        "all_ages = set(term for sublist in d['ages_split'] for term in sublist if term)\n",
        "all_communities = set(term for sublist in d['communities_split'] for term in sublist if term)\n",
        "all_ethnicities = set(term for sublist in d['ethnicities_split'] for term in sublist if term)\n",
        "all_religions = set(term for sublist in d['religions_split'] for term in sublist if term)\n",
        "\n",
        "# dummy code each population\n",
        "\n",
        "for a in all_ages:\n",
        "    age_snake_case = to_snake_case(a)\n",
        "    d[age_snake_case] = d['ages'].apply(lambda i: 1 if a in i else 0)\n",
        "\n",
        "for c in all_communities:\n",
        "    community_snake_case = to_snake_case(c)\n",
        "    d[community_snake_case] = d['communities'].apply(lambda i: 1 if c in i else 0)\n",
        "\n",
        "for e in all_ethnicities:\n",
        "    ethnicity_snake_case = to_snake_case(e)\n",
        "    d[ethnicity_snake_case] = d['ethnicities'].apply(lambda i: 1 if e in i else 0)\n",
        "\n",
        "for r in all_religions:\n",
        "    religion_snake_case = to_snake_case(r)\n",
        "    d[religion_snake_case] = d['religions'].apply(lambda i: 1 if r in i else 0)\n",
        "\n",
        "# drop '*_split' columns\n",
        "\n",
        "d = d.drop([\n",
        "            'ages_split',\n",
        "            'communities_split',\n",
        "            'ethnicities_split',\n",
        "            'religions_split',\n",
        "            ],\n",
        "            axis = 1,\n",
        "            #inplace = True,\n",
        "            )\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ooJQpxoApY9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Disambiguate insurers**"
      ],
      "metadata": {
        "id": "7-XUi75kv98J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve OpenAI API key\n",
        "\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "# define system prompt\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are an expert at recognizing and separating insurer names with commas.\n",
        "\"\"\"\n",
        "\n",
        "# define prompt template\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "Please separate the following run-together insurer names with commas:\n",
        "\n",
        "{input_text}\n",
        "\n",
        "Ensure the insurers are properly separated by commas. Return only the comma-separated insurer names without additional text or characters such as newlines.\n",
        "\n",
        "If there are no insurer names in the input text, return only a single period: '.'\n",
        "\"\"\"\n",
        "\n",
        "# transform, inspect\n",
        "\n",
        "d = transform_text_with_gpt(\n",
        "                            d,\n",
        "                            'insurance_raw',\n",
        "                            'insurance',\n",
        "                            system_prompt,\n",
        "                            prompt_template,\n",
        "                            )\n",
        "\n",
        "# drop 'raw' col\n",
        "\n",
        "d = d.drop(\n",
        "           'insurance_raw',\n",
        "           axis = 1,\n",
        "           )\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tIRoUAKcjR58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Disambiguate specialties**"
      ],
      "metadata": {
        "id": "dhFhF2b6xSZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve OpenAI API key\n",
        "\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "# define system prompt\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are an expert at recognizing and separating mental health counseling specialties (symptoms, special populations, etc.) with commas.\n",
        "\"\"\"\n",
        "\n",
        "# define prompt template\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "Please separate the following mental health counseling specialties with commas:\n",
        "\n",
        "{input_text}\n",
        "\n",
        "Ensure the specialties are properly separated by commas. Return only the comma-separated specialties without additional text or characters such as newlines.\n",
        "\n",
        "If there are no specialties in the input text, return only a single period: '.'\n",
        "\"\"\"\n",
        "\n",
        "# transform, inspect\n",
        "\n",
        "d = transform_text_with_gpt(\n",
        "                            d,\n",
        "                            'specialties_raw',\n",
        "                            'specialties',\n",
        "                            system_prompt,\n",
        "                            prompt_template,\n",
        "                            )\n",
        "\n",
        "# drop 'raw' col\n",
        "\n",
        "d = d.drop(\n",
        "           'specialties_raw',\n",
        "           axis = 1,\n",
        "           )\n"
      ],
      "metadata": {
        "id": "x5gtVLIqxP0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extract accreditations**"
      ],
      "metadata": {
        "id": "Mss0wPcjwWHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve OpenAI API key\n",
        "\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "# define system prompt\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are an expert at recognizing the certificates, degrees, and other accreditations of mental health professionals.\n",
        "\"\"\"\n",
        "\n",
        "# define prompt template\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "Please review this text and gather all certificates, degrees, and other accreditations of mental health professionals:\n",
        "\n",
        "{input_text}\n",
        "\n",
        "Return all certificates, degrees, and other accreditations of mental health professionals separated by commas. Do not\n",
        "add additional text or characters such as newlines.\n",
        "\n",
        "If there are no accreditations in the input text, return only a single period: '.'\n",
        "\"\"\"\n",
        "\n",
        "# transform, inspect\n",
        "\n",
        "d = transform_text_with_gpt(\n",
        "                            d,\n",
        "                            'image_alt_text',\n",
        "                            'accreditations',\n",
        "                            system_prompt,\n",
        "                            prompt_template,\n",
        "                            )\n"
      ],
      "metadata": {
        "id": "wVJS42KvwVZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Housekeeping**"
      ],
      "metadata": {
        "id": "kUyU-38DyUHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d = d.reindex(\n",
        "              columns = [\n",
        "                         'MHP ID',\n",
        "                         'name',\n",
        "                         'pronouns',\n",
        "                         'accreditations',\n",
        "                         'practice_name',\n",
        "                         'profile_url',\n",
        "                         'image_url',\n",
        "                         'image_alt_text',\n",
        "                         'place_name',\n",
        "                         'description',\n",
        "                         'at_a_glance',\n",
        "                         'qualifications',\n",
        "                         #'client_focus',\n",
        "                         #'ages',\n",
        "                         #'communities',\n",
        "                         #'ethnicities',\n",
        "                         #'religions',\n",
        "                        'specialties',\n",
        "                        'types_of_therapy',\n",
        "                        'insurance',\n",
        "                        'fees',\n",
        "                        'individual_fee',\n",
        "                        'couple_fee',\n",
        "                        'sliding_scale',\n",
        "                        'availability',\n",
        "                        'years_in_practice',\n",
        "                        'licensed_by_state',\n",
        "                        'license_number',\n",
        "                        'toddler',\n",
        "                        'children_(6_to_10)',\n",
        "                        'preteen',\n",
        "                        'teen',\n",
        "                        'adults',\n",
        "                        'elders_(65+)',\n",
        "                        'single_mother',\n",
        "                        'couples',\n",
        "                        'family',\n",
        "                        'racial_justice_allied',\n",
        "                        'hispanic_and_latino',\n",
        "                        'sex_worker_allied',\n",
        "                        'hiv_/_aids_allied',\n",
        "                        'immuno-disorders',\n",
        "                        'gay_allied',\n",
        "                        #'.',\n",
        "                        'bisexual_allied',\n",
        "                        'lesbian_allied',\n",
        "                        'non-binary_allied',\n",
        "                        'queer_allied',\n",
        "                        'intersex_allied',\n",
        "                        'transgender_allied',\n",
        "                        #'black_and_african_american',\n",
        "                        #'christian_____children_(6_to_10)',\n",
        "                        #'elders_(65+)____individuals',\n",
        "                        #'couples_____deaf_allied',\n",
        "                        #'christian_____adults',\n",
        "\n",
        "                        #'adults____individuals_____bisexual_allied',\n",
        "                        #'transgender_allied____black_and_african_american',\n",
        "                        #'immuno-disorders__i_also_speak_american_sign_langu__(asl)____christian',\n",
        "                        #'transgender_allied____christian',\n",
        "                        #'christian_____toddler',\n",
        "                        #'elders_(65+)____individuals____christian',\n",
        "                        #'group_____bisexual_allied',\n",
        "                        #'group____christian',\n",
        "                        #'family____christian',\n",
        "                        #'adults____individuals',\n",
        "                        #'christian_____adults____individuals_____single_mother____black_and_african_american____christian',\n",
        "                        #'hispanic_and_latino____christian',\n",
        "\n",
        "                        #'christian_____teen',\n",
        "                        #'elders_(65+)____individuals_____bisexual_allied',\n",
        "                          ])\n",
        "\n",
        "d.rename(\n",
        "         columns = {\n",
        "                    'children_(6_to_10)': 'children_6_to_10',\n",
        "                    'elders_(65+)': 'elders_65_plus',\n",
        "                    'hiv_/_aids_allied': 'hiv_aids_allied',\n",
        "                    'immuno-disorders': 'immuno_disorders',\n",
        "                    }, inplace = True,\n",
        "            )\n",
        "d.info()\n",
        "d.head(3)"
      ],
      "metadata": {
        "id": "AZlnGKNxygR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%pwd\n",
        "%cd ../../outputs/tables\n",
        "\n",
        "d.to_excel(\n",
        "           'd_html.xlsx',\n",
        "           index = False,\n",
        "           )"
      ],
      "metadata": {
        "id": "zSzMIRPoLIpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> End of mhp_annotate_iaa_append.ipynb"
      ],
      "metadata": {
        "id": "LPjC7kTtfa50"
      }
    }
  ]
}