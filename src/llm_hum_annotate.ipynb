{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pG6vcc1FQyh2"
   },
   "source": [
    "# llm-expert human co-annotation / integration TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IV7B3DTqRCCb"
   },
   "source": [
    "Performs qualitative deductive coding consistent with the [CHALET](https://arxiv.org/abs/2405.05758) (**C**ollaborative **H**uman-LLM **A**na**L**ysis for **E**mpowering Conceptualization in Quali**T**ative Research) approach. Requires Ollama and/or OpenAI API key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_xtxjHnRGgu"
   },
   "source": [
    "> llm_hum_annotate.ipynb<br>\n",
    "> Simone J. Skeen x Claude Code (02-05-2026)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMzbQWcLnD3k"
   },
   "source": [
    "### Prepare\n",
    "Installs, imports, requisite packages; customizes outputs.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6Rq0_Hj0lIb"
   },
   "source": [
    "**Install**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t0_sFYCXmuGE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%pip install -r ../requirements.txt\n",
    "%pip install irrCAC\n",
    "%pip install ollama\n",
    "%pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1SoN_HI0lId"
   },
   "source": [
    "**Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8zJyDOUnTOa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import ollama\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from irrCAC.raw import CAC\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "pd.set_option(\n",
    "    'display.max_columns',\n",
    "    None,\n",
    "    )\n",
    "\n",
    "pd.set_option(\n",
    "    'display.max_rows',\n",
    "    None,\n",
    "    )\n",
    "\n",
    "warnings.simplefilter(\n",
    "    action = 'ignore',\n",
    "    category = FutureWarning,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKXbAwfN0lIf"
   },
   "source": [
    "**Set env variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "euvcHEZgq5mS"
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory to project root; add src/ to path\n",
    "\n",
    "if os.path.basename(os.getcwd()) == 'src':\n",
    "    os.chdir('..')\n",
    "\n",
    "sys.path.insert(0, 'src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qualitative import (\n",
    "    load_annotation_config,\n",
    "    build_prompt_llama,\n",
    "    build_prompts_per_code,\n",
    "    code_texts_deductively_llama,\n",
    "    code_texts_deductively_gpt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MQLVYNIpRKg"
   },
   "outputs": [],
   "source": [
    "d = pd.read_excel(\n",
    "    'data/inputs/d_cycle_3_sjs.xlsx', ### d_cycle_3_sjs - IAA comparison w/ GPT-4o\n",
    "    index_col = [0],\n",
    "    )\n",
    "\n",
    "# replace ' ' w/ NaN\n",
    "\n",
    "d[[\n",
    "    'agnt', 'afrm',\n",
    "    'brdn', 'fitt',\n",
    "    'just', 'prbl',\n",
    "    'rbnd', 'refl',\n",
    "    ]] = d[[\n",
    "        'agnt', 'afrm',\n",
    "        'brdn', 'fitt',\n",
    "        'just', 'prbl',\n",
    "        'rbnd', 'refl',\n",
    "        ]].replace(\n",
    "            r'^\\s*$',\n",
    "            np.nan,\n",
    "            regex = True,\n",
    "            )\n",
    "\n",
    "# replace NaN w/ 0\n",
    "\n",
    "d[[\n",
    "    'agnt', 'afrm',\n",
    "    'brdn', 'fitt',\n",
    "    'just', 'prbl',\n",
    "    'rbnd', 'refl',\n",
    "    ]] = d[[\n",
    "        'agnt', 'afrm',\n",
    "        'brdn', 'fitt',\n",
    "        'just', 'prbl',\n",
    "        'rbnd', 'refl',\n",
    "        ]].apply(\n",
    "            pd.to_numeric,\n",
    "            downcast = 'integer',\n",
    "            )\n",
    "\n",
    "d.fillna(\n",
    "    0,\n",
    "    inplace = True,\n",
    "    )\n",
    "\n",
    "# texts: delete '<|PII|>' pseudoword\n",
    "\n",
    "texts = ['text']\n",
    "pseudoword_tokens = [\n",
    "    #'<SPL>',\n",
    "    '<|PII|>', ### 1/10: remove from 'rtnl' prior to training\n",
    "    ]\n",
    "\n",
    "for t in texts:\n",
    "    d[t] = d[t].replace(\n",
    "        pseudoword_tokens,\n",
    "        ' ',\n",
    "        regex = True,\n",
    "        )\n",
    "\n",
    "# rationales: replace NaN w/ '.'\n",
    "\n",
    "rationales = [\n",
    "    'rtnl',\n",
    "    #'afrm_llm_rtnl',\n",
    "    #'agnt_llm_rtnl',\n",
    "    #'fitt_llm_rtnl',\n",
    "    #'just_llm_rtnl',\n",
    "    #'refl_llm_rtnl',\n",
    "    'note',\n",
    "              ]\n",
    "\n",
    "for r in rationales:\n",
    "    d[r] = d[r].astype(str)\n",
    "    d[r] = d[r].str.replace(\n",
    "        r'0',\n",
    "        '.',\n",
    "        regex = True,\n",
    "        )\n",
    "\n",
    "# inspect\n",
    "\n",
    "d.info()\n",
    "d.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "Enables human-LLM deductive coding: human-specified per-tag prompts, JSON-.xlsx structured outputs.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hHjuQXrAqLE"
   },
   "source": [
    "#### Llama 3.2: local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLNNLfpZI5Tw"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "config = load_annotation_config()\n",
    "\n",
    "# build Llama prompt for 'refl'\n",
    "\n",
    "refl_prompt = build_prompt_llama(config, 'refl')\n",
    "\n",
    "# locally hosted Llama endpoint\n",
    "\n",
    "llama_endpoint = 'http://localhost:11434/api/generate'\n",
    "\n",
    "# classify texts and update df\n",
    "\n",
    "d = code_texts_deductively_llama(\n",
    "    d,\n",
    "    alias = 'refl',\n",
    "    text_column = 'text',\n",
    "    endpoint_url = llama_endpoint,\n",
    "    prompt_template = refl_prompt,\n",
    "    model_name = 'llama3',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT-4o: OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define which codes to annotate\n",
    "\n",
    "gpt_codes = ['afrm', 'agnt', 'fitt', 'just', 'rbnd', 'refl']\n",
    "#gpt_codes = ['afrm', 'agnt', 'fitt', 'frtn', 'just', 'rbnd', 'refl']\n",
    "\n",
    "# build all GPT prompts from YAML config\n",
    "\n",
    "prompts_per_code = build_prompts_per_code(config, gpt_codes, backend = 'gpt')\n",
    "\n",
    "# annotate df\n",
    "\n",
    "d = code_texts_deductively_gpt(\n",
    "    d,\n",
    "    prompts_per_code,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKFrQ9vU0lIo"
   },
   "outputs": [],
   "source": [
    "# inspect\n",
    "\n",
    "#print(d)\n",
    "d.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "#d.to_excel('data/outputs/d_cycle_3_sjs_gpt.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DuSQ858FR2Ab"
   },
   "source": [
    "**Inter-coder reliability: Cohen's $\\kappa$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d = pd.read_excel(\n",
    "#    'data/outputs/d_cycle_3_sjs_gpt.xlsx',\n",
    "#    index_col = [0],\n",
    "#    )\n",
    "\n",
    "#print(d.columns)\n",
    "\n",
    "# drop NaN\n",
    "\n",
    "d = d.dropna(subset = [\n",
    "    'afrm_gpt',\n",
    "    'agnt_gpt',\n",
    "    'fitt_gpt',\n",
    "    #'frtn_gpt',\n",
    "    'just_gpt',\n",
    "    'rbnd_gpt',\n",
    "    'refl_gpt',\n",
    "    ]\n",
    "             )\n",
    "\n",
    "# inspect\n",
    "\n",
    "d.info()\n",
    "d.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zt2p47yQRyrb"
   },
   "outputs": [],
   "source": [
    "# define kappa fx\n",
    "\n",
    "def calculate_kappa(d, col1, col2):\n",
    "    return cohen_kappa_score(d[col1], d[col2])\n",
    "\n",
    "col_pairs = [\n",
    "    ('afrm', 'afrm_gpt'),\n",
    "    ('agnt', 'agnt_gpt'),\n",
    "    ('fitt', 'fitt_gpt'),\n",
    "    #('brdn', 'frtn_gpt'),\n",
    "    ('just', 'just_gpt'),\n",
    "    ('rbnd', 'rbnd_gpt'),\n",
    "    ('refl', 'refl_gpt'),\n",
    "    ]\n",
    "\n",
    "# initialize dict\n",
    "\n",
    "kappa_results = {}\n",
    "\n",
    "# % agreement loop\n",
    "\n",
    "#agreement = d['afrm'] == d['afrm_gpt']\n",
    "#percent_agreement = (agreement.sum() / len(d)) * 100\n",
    "#print(\"Percent Agreement:\", percent_agreement)\n",
    "\n",
    "# function to compute percent agreement\n",
    "\n",
    "def calculate_percent_agreement(df, col_pairs):\n",
    "    results = {}\n",
    "    for col1, col2 in col_pairs:\n",
    "        agreement = df[col1] == df[col2]\n",
    "        percent_agreement = (agreement.sum() / len(df)) * 100\n",
    "        results[f\"{col1} & {col2}\"] = percent_agreement\n",
    "    return results\n",
    "\n",
    "# compute % agreement\n",
    "\n",
    "percent_agreement_results = calculate_percent_agreement(d, col_pairs)\n",
    "\n",
    "# print results\n",
    "\n",
    "for pair, percent in percent_agreement_results.items():\n",
    "    print(f\"Percent agreement for {pair}: {percent:.2f}%\")\n",
    "\n",
    "# kappa loop\n",
    "\n",
    "for col1, col2 in col_pairs:\n",
    "    kappa = calculate_kappa(d, col1, col2)\n",
    "    kappa_results[f'{col1} and {col2}'] = kappa\n",
    "\n",
    "for pair, kappa in kappa_results.items():\n",
    "    print(f\"Cohen's Kappa for {pair}: {kappa:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_dal = pd.read_excel('data/outputs/d_cycle_3_dal.xlsx', index_col = [0])\n",
    "d_dal.columns = [f'{col}_dal' for col in d_dal.columns]\n",
    "\n",
    "d_sjs = pd.read_excel('data/outputs/d_cycle_3_sjs.xlsx', index_col = [0])\n",
    "d_sjs.columns = [f'{col}_sjs' for col in d_sjs.columns]\n",
    "\n",
    "# merge\n",
    "\n",
    "d = pd.merge(\n",
    "    d_dal,\n",
    "    d_sjs,\n",
    "    left_index = True,\n",
    "    right_index = True,\n",
    "    )\n",
    "\n",
    "targets = [\n",
    "    'afrm_dal', 'afrm_sjs',\n",
    "    'agnt_dal', 'agnt_sjs',\n",
    "#    'dmnd_dal', 'dmnd_sjs',\n",
    "    'fitt_dal', 'fitt_sjs',\n",
    "#    'frtn_dal', 'frtn_sjs',\n",
    "    'just_dal', 'just_sjs',\n",
    "    'prbl_dal', 'prbl_sjs',\n",
    "    'rbnd_dal', 'rbnd_sjs',\n",
    "    'refl_dal', 'refl_sjs',\n",
    "    ]\n",
    "\n",
    "d[targets] = d[targets].apply(\n",
    "    pd.to_numeric,\n",
    "    errors = 'coerce',\n",
    "    )\n",
    "\n",
    "d[targets] = d[targets].fillna(0)\n",
    "\n",
    "d.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2eBnAhcDt6Wp"
   },
   "outputs": [],
   "source": [
    "# convert relevant columns to numeric type before calculating kappa\n",
    "\n",
    "for col in ['afrm_dal', 'afrm_sjs', 'agnt_dal', 'agnt_sjs', 'brdn_dal', 'brdn_sjs', 'fitt_dal', 'fitt_sjs', 'just_dal', 'just_sjs', 'rbnd_dal', 'rbnd_sjs', 'refl_dal', 'refl_sjs']:\n",
    "    d[col] = pd.to_numeric(d[col], errors='coerce')  # 'coerce' handles non-numeric values by setting them to NaN\n",
    "\n",
    "col_pairs = [\n",
    "    ('afrm_dal', 'afrm_sjs'),\n",
    "    ('agnt_dal', 'agnt_sjs'),\n",
    "    ('fitt_dal', 'fitt_sjs'),\n",
    "#    ('frtn_dal', 'frtn_sjs'),\n",
    "    ('just_dal', 'just_sjs'),\n",
    "    ('rbnd_dal', 'rbnd_sjs'),\n",
    "    ('refl_dal', 'refl_sjs'),\n",
    "    ]\n",
    "\n",
    "#d = d.dropna(subset=[col1, col2] for col1, col2 in col_pairs)\n",
    "\n",
    "# compute % agreement\n",
    "\n",
    "percent_agreement_results = calculate_percent_agreement(d, col_pairs)\n",
    "\n",
    "# print results\n",
    "\n",
    "for pair, percent in percent_agreement_results.items():\n",
    "    print(f\"Percent agreement for {pair}: {percent:.2f}%\")\n",
    "\n",
    "# kappa loop\n",
    "\n",
    "for col1, col2 in col_pairs:\n",
    "    kappa = calculate_kappa(d, col1, col2)\n",
    "    kappa_results[f'{col1} and {col2}'] = kappa\n",
    "\n",
    "for pair, kappa in kappa_results.items():\n",
    "    print(f\"Cohen's Kappa for {pair}: {kappa:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flag disagreements** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1U3ee6SkRyzL"
   },
   "outputs": [],
   "source": [
    "# flag disagreements Fx\n",
    "\n",
    "def encode_disagreements(row):\n",
    "    return 1 if row[0] != row[1] else 0\n",
    "\n",
    "col_dis = [\n",
    "    ('afrm', 'afrm_gpt', 'afrm_dis'),\n",
    "    ('agnt', 'agnt_gpt', 'agnt_dis'),\n",
    "    ('fitt', 'fitt_gpt', 'fitt_dis'),\n",
    "    ('frtn', 'frtn_gpt', 'frtn_dis'),\n",
    "    ('just', 'just_gpt', 'just_dis'),\n",
    "    ('rbnd', 'rbnd_gpt', 'rbnd_dis'),\n",
    "    ('refl', 'refl_gpt', 'refl_dis'),\n",
    "  ]\n",
    "\n",
    "for col1, col2, dis_col in col_dis:\n",
    "    d[dis_col] = d[[col1, col2]].apply(\n",
    "        encode_disagreements,\n",
    "        axis = 1,\n",
    "        )\n",
    "\n",
    "# export\n",
    "\n",
    "d.to_excel('data/outputs/d_pilot_coded_iaa.xlsx')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "FvW_mxkE0lIk",
    "0TXsMF50oDSi",
    "LrgYlrmo1OUW",
    "I6V00vzh2Na1",
    "zJvBy9EZy6yC",
    "zXYJT6i9pSPf",
    "6hHjuQXrAqLE",
    "3artTCRiIwTl"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
