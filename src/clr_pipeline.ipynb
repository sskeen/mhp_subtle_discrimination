{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c639535",
   "metadata": {},
   "source": [
    "## mhp ground-up refactor / headers etc. TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e0476c",
   "metadata": {},
   "source": [
    "### 1. Prepare\n",
    "Installs, imports, requisite packages; customizes outputs.\n",
    "***\n",
    "> **Dependencies:** Install via `%pip install -r requirements.txt` from project root before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaa9535",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dff1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import(\n",
    "    f1_score, \n",
    "    matthews_corrcoef, \n",
    "    average_precision_score,\n",
    "    ) \n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "pd.set_option(\n",
    "    'display.max_columns',\n",
    "    None,\n",
    "    )\n",
    "\n",
    "pd.set_option(\n",
    "    'display.max_rows',\n",
    "    None,\n",
    "    )\n",
    "\n",
    "for c in (FutureWarning, UserWarning):\n",
    "    warnings.simplefilter(\n",
    "        action = 'ignore',\n",
    "        category = c,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8d1e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b7168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import annotated `d_train`\n",
    "\n",
    "d = pd.read_excel(\"d_annotated_prelim.xlsx\", index_col = [0])\n",
    "\n",
    "d.info()\n",
    "d.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b6bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_lg --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d207a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spaCy NER pipeline\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# redact named entities\n",
    "\n",
    "PII_LABELS = {\n",
    "    \"PERSON\", \"NORP\", \"FAC\", \"ORG\", \n",
    "    \"GPE\", \"LOC\", \"PRODUCT\", \"EVENT\",\n",
    "    }\n",
    "\n",
    "def redact_named_entities(doc) -> str:\n",
    "    '''\n",
    "    Replaces all spaCy-recognized (configurable upstream) \n",
    "    PII_LABELS with `<PII>` pseudoword token.\n",
    "    '''\n",
    "    chars = list(doc.text)\n",
    "    for ent in sorted(doc.ents, key = lambda e: e.start_char, reverse = True):\n",
    "        if ent.label_ not in PII_LABELS:\n",
    "            continue\n",
    "        chars[ent.start_char : ent.end_char] = list(\"<PII>\")\n",
    "    return \"\".join(chars)\n",
    "\n",
    "        ### TODO: SJS 2/3: certain synthetic prospective client names (e.g. Darius) are not automatically redacted\n",
    "\n",
    "texts = d['text'].astype(str).tolist()\n",
    "d['text'] = [redact_named_entities(doc) for doc in nlp.pipe(texts)]\n",
    "\n",
    "# remove stray manual redactions (pilot / Phase 1)\n",
    "\n",
    "manual_redactions = [\n",
    "    \"[PATIENT NAME]\", \"[PHONE NUMBER]\",\n",
    "    \"[MHP NAME]\", \"[CITY]\", \"[URL]\",\n",
    "    ]\n",
    "\n",
    "for m in manual_redactions:\n",
    "    d['text'] = d['text'].str.replace(\n",
    "        m, \n",
    "        \" \", \n",
    "        regex = False,\n",
    "        )\n",
    "\n",
    "# remove numerals, newlines\n",
    "\n",
    "d['text'] = d['text'].str.replace(\n",
    "    r\"[\\d\\r\\n]+\", \n",
    "    \" \", \n",
    "    regex = True,\n",
    "    )\n",
    "\n",
    "# replace NaN with 0 in `target` varlist\n",
    "\n",
    "labels = [\n",
    "    \"afrm\", \"agnt\", \"brdn\", \"dmnd\", \"fitt\", \n",
    "    \"just\", \"prbl\", \"rbnd\", \"refl\",\n",
    "    ]\n",
    "\n",
    "for l in labels:\n",
    "    d[l] = pd.to_numeric(d[l], errors = \"coerce\").fillna(0).astype(int)\n",
    "\n",
    "#d.info()\n",
    "#d.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2f756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen `trgt` indicator: dummy code pos(1) rows\n",
    "\n",
    "d['trgt'] = d[[\n",
    "    'afrm', 'agnt',\n",
    "    'fitt', 'refl',\n",
    "    ]].apply(lambda row: 1 if any(row) else 0, axis = 1)\n",
    "\n",
    "def augment(df):\n",
    "    '''\n",
    "    Detects pos(1) `trgt` rows; duplicates as new row; replaces new row `text` \n",
    "    with expert annotator-curated `rtnl` text to augment training instance.\n",
    "    '''\n",
    "    new_rows = []\n",
    "    for index, row in df.iterrows():\n",
    "        if row['trgt'] > 0:\n",
    "            new_row = row.copy()\n",
    "            new_row['text'] = row['rtnl']\n",
    "            new_rows.append((index + 0.5, new_row))\n",
    "\n",
    "    if not new_rows:\n",
    "        return df\n",
    "\n",
    "    aug_df = pd.DataFrame(\n",
    "        [row for _, row in new_rows],\n",
    "        index = [idx for idx, _ in new_rows],\n",
    "        )\n",
    "    df = pd.concat([df, aug_df])\n",
    "    df = df.sort_index(kind = \"stable\").reset_index(drop = True)\n",
    "    return df\n",
    "\n",
    "d = augment(d.copy())\n",
    "\n",
    "# gen `agmt` indicator: dummy code augmented rows\n",
    "\n",
    "d['agmt'] = 0\n",
    "t_indices = d['rtnl'].apply(lambda i: isinstance(i, str))\n",
    "d.loc[t_indices.shift(1, fill_value = False), 'agmt'] = 1\n",
    "\n",
    "# remove `rtnl` construct salience delineator pseudoword tokens (parent / Phase 2) / `<PII>` redactions\n",
    "\n",
    "salience_delineators = [\n",
    "    \"<PII>\", \"<|PII|>\", \"<|AFRM|>\", \"<|AGNT|>\", \"<|BRDN|>\", \"<|DMND|>\",  \n",
    "    \"<|FITT|>\", \"<|JUST|>\", \"<|PRBL|>\", \"<|RBND|>\", \"<|REFL|>\",\n",
    "    ]\n",
    "\n",
    "for s in salience_delineators:\n",
    "    d['text'] = d['text'].str.replace(\n",
    "        s, \n",
    "        \" \", \n",
    "        regex = False,\n",
    "        )\n",
    "\n",
    "d.shape\n",
    "d.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b75e0ba",
   "metadata": {},
   "source": [
    "#### Compute weights ($w$): inverse class ($c$) freq: $w_c = N / (2 * n_c)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8c6b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = {}\n",
    "\n",
    "for l in labels:\n",
    "    value_counts = d[l].value_counts()\n",
    "    n_pos = value_counts.get(1, 0)\n",
    "    n_neg = value_counts.get(0, 0)\n",
    "    w_pos = round(len(d) / (2 * n_pos), 4) if n_pos > 0 else 0\n",
    "    w_neg = round(len(d) / (2 * n_neg), 4) if n_neg > 0 else 0\n",
    "    class_weights[l] = {\n",
    "        'w_pos': w_pos,\n",
    "        'w_neg': w_neg,\n",
    "        }\n",
    "\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e669b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "        ### TODO: SJS 2/3: harmonize w/ preregistered PAP methods: https://osf.io/wgu8q/\n",
    "\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "SEED = 56\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 512\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len = MAX_LEN):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            str(self.texts[idx]),\n",
    "            max_length = self.max_len,\n",
    "            padding = \"max_length\",\n",
    "            truncation = True,\n",
    "            return_tensors = \"pt\",\n",
    "            )\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(self.labels[idx], dtype = torch.long),\n",
    "            }\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c468e4af",
   "metadata": {},
   "source": [
    "#### held-out test set split / augmented-instance mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pf7wqiacdxq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map augmented rows â†’ parent originals via sequential ordering in d\n",
    "\n",
    "orig_d_positions = d.index[d['agmt'] == 0].tolist()\n",
    "augm_d_positions = d.index[d['agmt'] == 1].tolist()\n",
    "\n",
    "d_orig = d.loc[orig_d_positions].reset_index(drop = True)\n",
    "d_augm = d.loc[augm_d_positions].copy()\n",
    "\n",
    "# for each augmented row, record parent's idx in d_orig\n",
    "\n",
    "d_pos_to_orig_idx = {pos: i for i, pos in enumerate(orig_d_positions)}\n",
    "parent_indices = []\n",
    "for ai in augm_d_positions:\n",
    "    parent_pos = max(op for op in orig_d_positions if op < ai)\n",
    "    parent_indices.append(d_pos_to_orig_idx[parent_pos])\n",
    "\n",
    "d_augm['_parent_idx'] = parent_indices\n",
    "d_augm = d_augm.reset_index(drop = True)\n",
    "\n",
    "# stratified holdout test set - originals (non-augmented) only\n",
    "\n",
    "train_val_idx, test_idx = train_test_split(\n",
    "    np.arange(len(d_orig)),\n",
    "    test_size = 0.15,\n",
    "    random_state = SEED,\n",
    "    stratify = d_orig['trgt'],\n",
    "    )\n",
    "\n",
    "d_test = d_orig.iloc[test_idx].reset_index(drop = True)\n",
    "\n",
    "# parse augmented rows to those whose parent is in train/val\n",
    "\n",
    "train_val_set = set(train_val_idx)\n",
    "d_augm_tv = d_augm[d_augm['_parent_idx'].isin(train_val_set)].reset_index(drop = True)\n",
    "\n",
    "print(f\"Train/val originals:     {len(train_val_idx)}\")\n",
    "print(f\"Test originals:          {len(test_idx)}\")\n",
    "print(f\"Augmented (train only):  {len(d_augm_tv)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb745bf2",
   "metadata": {},
   "source": [
    "#### train / evaluate helper fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w5ys0ws25r7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_texts, train_labels, weights, epochs = EPOCHS):\n",
    "    set_seed(SEED)\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, num_labels = 2,\n",
    "        ).to(device)\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(weight = weights.to(device))\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr = 2e-5)\n",
    "\n",
    "    train_ds = TextDataset(train_texts, train_labels, tokenizer)\n",
    "    train_loader = DataLoader(train_ds, batch_size = BATCH_SIZE, shuffle = True)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                input_ids = batch['input_ids'].to(device),\n",
    "                attention_mask = batch['attention_mask'].to(device),\n",
    "                )\n",
    "            loss = loss_fn(outputs.logits, batch['label'].to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate(model, eval_texts, eval_labels):\n",
    "    eval_ds = TextDataset(eval_texts, eval_labels, tokenizer)\n",
    "    eval_loader = DataLoader(eval_ds, batch_size = BATCH_SIZE)\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_probs, all_labels = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            outputs = model(\n",
    "                input_ids = batch['input_ids'].to(device),\n",
    "                attention_mask = batch['attention_mask'].to(device),\n",
    "                )\n",
    "            probs = torch.softmax(outputs.logits, dim = 1)[:, 1]\n",
    "            preds = outputs.logits.argmax(dim = 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(batch['label'].numpy())\n",
    "\n",
    "    return {\n",
    "        'f1_macro': f1_score(all_labels, all_preds, average = 'macro', zero_division = 0),\n",
    "        'mcc': matthews_corrcoef(all_labels, all_preds),\n",
    "        'auprc': average_precision_score(all_labels, all_probs) if sum(all_labels) > 0 else 0.0,\n",
    "        }\n",
    "\n",
    "# 5-fold stratified CV per label + held-out test\n",
    "\n",
    "d_tv = d_orig.iloc[train_val_idx]\n",
    "skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = SEED)\n",
    "results = {}\n",
    "\n",
    "for label in labels:\n",
    "    print(f\"\\n{'=' * 50}\")\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"{'=' * 50}\")\n",
    "\n",
    "    w = class_weights[label]\n",
    "    weights = torch.tensor([w['w_neg'], w['w_pos']], dtype = torch.float)\n",
    "\n",
    "    fold_metrics = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(d_tv, d_tv[label])):\n",
    "\n",
    "        # training: fold originals + matching augmented rows\n",
    "\n",
    "        fold_train = d_tv.iloc[train_idx]\n",
    "        train_orig_set = set(train_val_idx[train_idx])\n",
    "        augm_fold = d_augm_tv[d_augm_tv['_parent_idx'].isin(train_orig_set)]\n",
    "        fold_train = pd.concat([fold_train, augm_fold], ignore_index = True)\n",
    "\n",
    "        # validation: fold originals only\n",
    "\n",
    "        fold_val = d_tv.iloc[val_idx]\n",
    "\n",
    "        model = train_model(\n",
    "            fold_train['text'].tolist(),\n",
    "            fold_train[label].tolist(),\n",
    "            weights,\n",
    "            )\n",
    "\n",
    "        metrics = evaluate(\n",
    "            model,\n",
    "            fold_val['text'].tolist(),\n",
    "            fold_val[label].tolist(),\n",
    "            )\n",
    "        fold_metrics.append(metrics)\n",
    "\n",
    "        print(f\"  Fold {fold + 1}: F1 = {metrics['f1_macro']:.4f}  MCC = {metrics['mcc']:.4f}  AUPRC = {metrics['auprc']:.4f}\")\n",
    "\n",
    "        del model\n",
    "        if hasattr(torch.mps, 'empty_cache'):\n",
    "            torch.mps.empty_cache()\n",
    "\n",
    "    # mean CV metrics\n",
    "\n",
    "    mean_metrics = {k: np.mean([m[k] for m in fold_metrics]) for k in fold_metrics[0]}\n",
    "    print(f\"  CV Mean: F1 = {mean_metrics['f1_macro']:.4f}  MCC = {mean_metrics['mcc']:.4f}  AUPRC = {mean_metrics['auprc']:.4f}\")\n",
    "\n",
    "    # final model: retrain on all train/val + augmented, evaluate on held-out test\n",
    "\n",
    "        ### TODO: SJS 2/3: _best-performing_ model graduates to held-out test set...\n",
    "\n",
    "    all_train = pd.concat([d_tv, d_augm_tv], ignore_index = True)\n",
    "    model = train_model(\n",
    "        all_train['text'].tolist(),\n",
    "        all_train[label].tolist(),\n",
    "        weights,\n",
    "        )\n",
    "\n",
    "    test_metrics = evaluate(\n",
    "        model,\n",
    "        d_test['text'].tolist(),\n",
    "        d_test[label].tolist(),\n",
    "        )\n",
    "    print(f\"  Test:    F1 = {test_metrics['f1_macro']:.4f}  MCC = {test_metrics['mcc']:.4f}  AUPRC = {test_metrics['auprc']:.4f}\")\n",
    "\n",
    "    results[label] = {'cv': mean_metrics, 'test': test_metrics}\n",
    "\n",
    "    del model\n",
    "    if hasattr(torch.mps, 'empty_cache'):\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "# summarize\n",
    "\n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(\"Summary\")\n",
    "print(f\"{'=' * 50}\")\n",
    "\n",
    "for label, res in results.items():\n",
    "    cv, test = res['cv'], res['test']\n",
    "    print(\n",
    "        f\"{label:>6s}  \"\n",
    "        f\"CV: F1 = {cv['f1_macro']:.4f} MCC = {cv['mcc']:.4f} AUPRC = {cv['auprc']:.4f}  |  \"\n",
    "        f\"Test: F1 = {test['f1_macro']:.4f} MCC = {test['mcc']:.4f} AUPRC = {test['auprc']:.4f}\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
